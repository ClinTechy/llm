<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Learning App</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Custom CSS for a nicer design */
        body {
            font-family: 'Inter', sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: flex-start;
            min-height: 100vh;
            background: linear-gradient(135deg, #0F172A 0%, #020617 100%); /* Deep dark gradient */
            color: #E2E8F0; /* Lighter off-white text */
            margin: 0;
            padding: 24px;
            box-sizing: border-box;
        }
        #app {
            background-color: #1E293B; /* Darker blue-gray for main app container */
            border-radius: 2rem; /* More rounded overall container */
            box-shadow: 0 25px 50px -12px rgba(0, 0, 0, 0.8), 0 10px 20px -5px rgba(0, 0, 0, 0.6); /* Stronger, deeper shadow */
            width: 100%;
            max-width: 1400px; /* Even wider for large tablets/desktops */
            overflow: hidden;
            border: 1px solid #334155; /* Subtle border for definition */
            display: none; /* Hidden by default until password is entered */
        }
        header {
            background: linear-gradient(90deg, #1A202C 0%, #2D3748 100%); /* Header gradient */
            padding: 2.5rem; /* More padding */
            border-bottom: 1px solid #334155;
            text-align: center;
        }
        h1 {
            font-size: 3.5rem; /* Larger title */
            font-weight: 800; /* Extra bold */
            color: #2DD4BF; /* Teal accent */
            text-shadow: 0 0 15px rgba(45, 212, 191, 0.5); /* Subtle glow */
            margin-bottom: 1rem;
        }
        .concept-description-container {
            padding: 2.5rem; /* Padding for the concept description */
            background-color: #1E293B; /* Match app background */
        }
        .main-content-area {
            padding: 2.5rem; /* More padding in main content */
            display: flex;
            flex-direction: column;
            gap: 2.5rem; /* More space between sections */
        }

        canvas {
            display: block;
            background-color: #0D1117; /* Very Dark Canvas */
            border-radius: 1rem; /* Rounded canvas */
            border: 2px solid #475569; /* More prominent border */
            box-shadow: inset 0 0 10px rgba(0, 0, 0, 0.5); /* Inner shadow for depth */
            width: 100%;
            height: auto;
            min-height: 350px; /* Larger min height for visualizer */
            max-height: 600px; /* Larger max height */
        }

        /* Buttons */
        .btn-primary, .btn-secondary, .btn-danger {
            padding: 0.9rem 2rem; /* More padding for buttons */
            border-radius: 0.75rem; /* Rounded buttons */
            font-weight: 600;
            transition: all 0.3s ease;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.4); /* Button shadow */
            font-size: 1rem; /* Larger button text */
        }
        .btn-primary:hover {
            background-color: #4F46E5;
            transform: translateY(-3px);
            box-shadow: 0 8px 15px rgba(0, 0, 0, 0.5);
        }
        .btn-secondary {
            background-color: #FBBF24; /* Amber */
            color: #1F2937;
            border: 1px solid #D97706;
        }
        .btn-secondary:hover {
            background-color: #D97706;
            transform: translateY(-3px);
            box-shadow: 0 8px 15px rgba(0, 0, 0, 0.5);
        }
        .btn-danger {
            background-color: #EF4444; /* Red */
            color: white;
            border: 1px solid #DC2626;
        }
        .btn-danger:hover {
            background-color: #DC2626;
            transform: translateY(-3px);
            box-shadow: 0 8px 15px rgba(0, 0, 0, 0.5);
        }

        /* Cards and Panels */
        .bg-gray-800 {
            background-color: #1F2937; /* Consistent card background */
            border-radius: 1.5rem; /* More rounded cards */
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3); /* Card shadow */
            border: 1px solid #374151;
        }
        .text-gray-100 { color: #F3F4F6; } /* Lighter headings */
        .text-gray-300 { color: #D1D5DB; } /* Lighter body text */
        .text-gray-400 { color: #9CA3AF; } /* Even lighter for descriptions */
        .text-teal-300 { color: #4FD1C5; } /* Teal for stats */
        .text-teal-400 { color: #2DD4BF; } /* Brighter teal for titles */

        /* Stats Cards */
        .stats-value { font-size: 2.5rem; font-weight: 700; line-height: 1.2; color: #2DD4BF; } /* Larger, teal stats */
        .stats-label { font-size: 1rem; color: #9CA3AF; margin-top: 0.5rem; }

        /* Resource Gauges */
        .resource-gauge-container {
            padding: 1.5rem;
            border-radius: 1.5rem;
        }
        .resource-label { font-size: 1rem; color: #9CA3AF; margin-bottom: 0.5rem; }
        .resource-bar { height: 25px; background-color: #4B5563; border-radius: 0.75rem; overflow: hidden; margin-top: 0.5rem; }
        .resource-bar > div { height: 100%; transition: width 0.5s ease-out; border-radius: 0.75rem; }
        #cpuBarVal { background-color: #FBBF24; } /* Amber */
        #ramBarVal { background-color: #60A5FA; } /* Blue */

        /* Context Display */
        .context-display {
            background-color: #0D1117;
            border: 2px solid #475569;
            border-radius: 1rem;
            padding: 1rem;
            height: 120px; /* Increased height */
            overflow-y: auto;
            font-size: 0.9rem; /* Larger font */
            line-height: 1.8;
            color: #A0AEC0;
            box-shadow: inset 0 0 8px rgba(0, 0, 0, 0.3);
        }
        .prompt-frag-text { color: #34D399; margin-right: 8px; font-weight: 700;} /* Brighter green, bolder */
        .output-token-text { color: #F472B6; margin-right: 8px; font-weight: 700;} /* Brighter pink, bolder */

        /* Inference Mode Toggle */
        .inference-mode-toggle {
            background-color: #1F2937;
            border-radius: 1.5rem;
            padding: 1.25rem 2rem;
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 2rem;
            border: 1px solid #374151;
        }
        .inference-mode-toggle label {
            font-size: 1.1rem; /* Larger toggle labels */
            font-weight: 600;
            color: #E2E8F0;
            cursor: pointer;
        }
        .inference-mode-toggle input[type="radio"] {
            transform: scale(1.4); /* Larger toggle radios */
            accent-color: #2DD4BF; /* Teal accent */
        }
        .inference-mode-toggle input[type="radio"]:checked + label {
            color: #2DD4BF; /* Highlight checked label with teal */
            font-weight: 700;
        }

        /* New Toggle Button Styles */
        .toggle-button-group {
            display: flex;
            border-radius: 0.75rem;
            overflow: hidden;
            border: 2px solid #475569; /* Border for the group */
            box-shadow: inset 0 2px 4px rgba(0, 0, 0, 0.3);
        }
        .toggle-button {
            padding: 0.8rem 1.8rem;
            font-weight: 600;
            font-size: 1rem;
            cursor: pointer;
            transition: all 0.2s ease;
            background-color: #2D3A4B; /* Default background */
            color: #94A3B8; /* Default text color */
            border: none; /* No individual button borders */
        }
        .toggle-button.active {
            background-color: #2DD4BF; /* Active background color (Teal) */
            color: #0D1117; /* Dark text for active button */
            box-shadow: inset 0 0 10px rgba(0, 0, 0, 0.4); /* Inner shadow for active */
        }
        .toggle-button:hover:not(.active) {
            background-color: #334155;
            color: #E2E8F0;
        }


        /* Tab Styles */
        .tab-buttons {
            display: flex;
            justify-content: center;
            margin-bottom: 2.5rem; /* More space below tabs */
            border-bottom: 2px solid #475569; /* Thicker border */
            flex-wrap: wrap;
            gap: 0.75rem; /* Gap between buttons */
        }
        .tab-button {
            background-color: #2D3A4B; /* Darker tab background */
            color: #94A3B8;
            padding: 1rem 1.6rem; /* More padding */
            border: 1px solid #475569;
            border-bottom: none;
            border-top-left-radius: 1rem;
            border-top-right-radius: 1rem;
            cursor: pointer;
            transition: all 0.2s ease;
            font-weight: 600;
            font-size: 1.05rem; /* Larger font for tabs */
            white-space: nowrap;
            box-shadow: 0 -4px 8px rgba(0, 0, 0, 0.2); /* Subtle shadow above tabs */
        }
        .tab-button.active {
            background-color: #0D1117; /* Active tab background */
            color: #2DD4BF; /* Teal for active tab */
            border-color: #2DD4BF; /* Teal border for active tab */
            border-bottom-color: transparent;
            transform: translateY(-4px); /* More pronounced lift for active tab */
            box-shadow: 0 -8px 15px rgba(0, 0, 0, 0.4);
        }
        .tab-button:not(.active):hover {
            background-color: #334155;
            color: #E2E8F0;
        }
        .tab-content {
            background-color: #0D1117; /* Content area background */
            border-radius: 1.5rem;
            border: 2px solid #475569;
            padding: 2.5rem; /* Increased padding */
            box-shadow: inset 0 0 15px rgba(0, 0, 0, 0.4); /* Inner shadow for content area */
        }

        /* Fieldset and Radio Grid */
        .fieldset-card {
            background-color: #1F2937;
            border: 1px solid #374151;
            border-radius: 1.25rem; /* Rounded fieldsets */
            padding: 1.5rem;
            box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
            margin-bottom: 1.5rem; /* Space between fieldsets */
        }
        .legend-title {
            font-size: 1.15rem; /* Larger legend title */
            font-weight: 700;
            color: #2DD4BF; /* Teal for legend */
            text-align: center;
            margin-bottom: 1rem;
            padding: 0 1rem;
        }
        .radio-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)); /* Wider columns for radios */
            gap: 1rem; /* More gap */
        }
        .radio-grid div {
            display: flex;
            align-items: center;
            background-color: #2D3A4B; /* Background for each radio item */
            padding: 0.8rem 1rem;
            border-radius: 0.75rem;
            border: 1px solid #475569;
            cursor: pointer;
            transition: all 0.2s ease;
        }
        .radio-grid div:hover {
            background-color: #334155;
            border-color: #2DD4BF; /* Teal border on hover */
        }
        .radio-grid input[type="radio"] {
            transform: scale(1.1); /* Slightly larger radio buttons */
            accent-color: #2DD4BF; /* Teal accent */
            margin-right: 0.6rem;
        }
        .radio-grid input[type="radio"]:checked + label {
            color: #2DD4BF; /* Highlight checked label */
            font-weight: 600;
        }
        .radio-grid label {
            font-size: 0.95rem; /* Slightly larger radio labels */
            color: #E2E8F0;
        }

        /* Sliders */
        .slider-custom {
            -webkit-appearance: none;
            appearance: none;
            height: 12px; /* Thicker slider */
            background: #475569;
            border-radius: 6px;
            outline: none;
            opacity: 0.9;
            transition: opacity .2s;
            box-shadow: inset 0 2px 4px rgba(0, 0, 0, 0.3);
        }
        .slider-custom::-webkit-slider-thumb {
            -webkit-appearance: none;
            appearance: none;
            width: 28px; /* Larger thumb */
            height: 28px; /* Larger thumb */
            border-radius: 50%;
            background: #2DD4BF; /* Teal thumb */
            cursor: pointer;
            transition: background .2s, transform .2s;
            margin-top: -8px; /* Center thumb vertically */
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.4);
            border: 2px solid #1F2937; /* Dark border for thumb */
        }
        .slider-custom::-webkit-slider-thumb:hover {
            background: #20B2AA; /* Darker teal on hover */
            transform: scale(1.05);
        }
        .slider-custom::-moz-range-thumb {
            width: 28px;
            height: 28px;
            border-radius: 50%;
            background: #2DD4BF;
            cursor: pointer;
            transition: background .2s, transform .2s;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.4);
            border: 2px solid #1F2937;
        }
        .slider-custom::-moz-range-thumb:hover {
            background: #20B2AA;
            transform: scale(1.05);
        }

        /* Accordion Group */
        .accordion-group {
            display: flex;
            flex-direction: column;
            gap: 1rem; /* Space between accordion items */
        }
        .accordion-item {
            background-color: #1F2937;
            border: 1px solid #374151;
            border-radius: 1rem;
            overflow: hidden;
            box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
        }
        .accordion-summary {
            padding: 1.25rem 1.5rem;
            font-weight: 700;
            color: #E2E8F0;
            background-color: #2D3A4B; /* Darker header for summary */
            border-bottom: 1px solid #374151;
            display: flex;
            justify-content: space-between;
            align-items: center;
            cursor: pointer;
            transition: all 0.2s ease;
            font-size: 1.05rem;
        }
        .accordion-item[open] > .accordion-summary {
            background-color: #334155; /* Even darker when open */
            border-bottom-color: #2DD4BF; /* Teal line when open */
            color: #2DD4BF;
        }
        .accordion-summary:hover {
            background-color: #334155;
        }
        .accordion-item[open] .accordion-summary::after {
            content: "−";
        }
        .accordion-summary::after {
            content: "+";
            font-size: 1.5em;
            margin-left: 1rem;
            color: #9CA3AF;
        }
        .accordion-item[open] .accordion-summary::after {
            color: #2DD4BF;
        }
        .accordion-content {
            padding: 1.5rem;
            background-color: #1F2937;
            color: #CBD5E1;
            font-size: 0.95rem;
            line-height: 1.7;
        }
        .accordion-content ul {
            list-style: disc;
            padding-left: 1.5rem;
        }
        .accordion-content ul ul {
            list-style: circle;
            padding-left: 1.25rem;
            margin-top: 0.5rem;
        }
        .accordion-content strong {
            color: #93C5FD; /* Lighter blue for emphasis */
        }
        .note {
            font-size: 0.85rem;
            color: #A0AEC0;
            margin-left: 0.8rem;
        }

        /* General Concept Description */
        .concept-description {
            font-size: 0.95rem;
            color: #A0AEC0;
            margin-top: 1rem;
            min-height: 35px;
            line-height: 1.6;
        }

        /* New layout for controls and stats section */
        .controls-stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); /* Flexible columns */
            gap: 1.5rem; /* Gap between cards */
        }

        /* Responsive Adjustments */
        @media (max-width: 1200px) { /* Large Tablet / Small Desktop */
            #app { max-width: 1000px; }
            h1 { font-size: 3rem; }
            .concept-description-container, .main-content-area { padding: 2rem; }
            canvas { min-height: 300px; max-height: 500px; }
            .btn-primary, .btn-secondary, .btn-danger { padding: 0.8rem 1.8rem; font-size: 0.95rem; }
            .stats-value { font-size: 2.2rem; }
            .stats-label { font-size: 0.9rem; }
            .resource-bar { height: 22px; }
            .context-display { height: 100px; font-size: 0.85rem; padding: 0.8rem; }
            .tab-button { padding: 0.9rem 1.4rem; font-size: 1rem; }
            .tab-content { padding: 2rem; }
            .fieldset-card { padding: 1.25rem; }
            .legend-title { font-size: 1.1rem; }
            .radio-grid { grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); }
            .slider-custom::-webkit-slider-thumb, .slider-custom::-moz-range-thumb { width: 24px; height: 24px; margin-top: -7px; }
            .accordion-summary, .accordion-content { font-size: 1rem; padding: 1.2rem 1.4rem; }
            .controls-stats-grid { grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); }
        }

        @media (max-width: 768px) { /* Tablet Portrait / Large Mobile */
            body { padding: 16px; }
            #app { border-radius: 1.5rem; }
            header { padding: 1.5rem; }
            h1 { font-size: 2.5rem; }
            .concept-description-container, .main-content-area { padding: 1.5rem; }
            canvas { min-height: 250px; max-height: 450px; }
            .btn-primary, .btn-secondary, .btn-danger { padding: 0.7rem 1.5rem; font-size: 0.9rem; }
            .controls { flex-direction: column; align-items: stretch; } /* This is for the internal buttons within a card */
            .controls button { margin-bottom: 0.5rem; }
            .stats-value { font-size: 1.8rem; }
            .stats-label { font-size: 0.85rem; }
            .resource-bar { height: 20px; }
            .context-display { height: 90px; font-size: 0.8rem; padding: 0.7rem; }
            .inference-mode-toggle { flex-direction: column; gap: 1rem; padding: 1rem; }
            .tab-buttons { flex-direction: column; gap: 0.5rem; margin-bottom: 1.5rem; }
            .tab-button { border-radius: 0.75rem; border-bottom: 1px solid #374151; font-size: 0.9rem; padding: 0.8rem 1.2rem; }
            .tab-button.active { border-bottom: 1px solid transparent; transform: translateY(-2px); }
            .tab-content { padding: 1.5rem; border-radius: 1rem; }
            .fieldset-card { padding: 1rem; border-radius: 1rem; margin-bottom: 1rem; }
            .legend-title { font-size: 1rem; margin-bottom: 0.75rem; }
            .radio-grid { grid-template-columns: 1fr; gap: 0.75rem; }
            .radio-grid div { padding: 0.5rem 0.75rem; border-radius: 0.5rem; }
            .radio-grid input[type="radio"] { transform: scale(1.05); margin-right: 0.5rem; }
            .radio-grid label { font-size: 0.9rem; }
            .slider-custom { height: 10px; }
            .slider-custom::-webkit-slider-thumb, .slider-custom::-moz-range-thumb { width: 20px; height: 20px; margin-top: -6px; }
            .accordion-summary, .accordion-content { font-size: 0.95rem; padding: 1rem 1.2rem; }
            .accordion-summary::after { font-size: 1.3em; margin-left: 0.8rem; }
            .concept-description { font-size: 0.85rem; margin-top: 0.8rem; }
            .accordion-content ul { padding-left: 1.25rem; }
            .accordion-content ul ul { padding-left: 1rem; }
            .controls-stats-grid { grid-template-columns: 1fr; } /* Stack vertically on small screens */
        }

        @media (max-width: 480px) { /* Small Mobile */
            body { padding: 10px; }
            header { padding: 1rem; }
            h1 { font-size: 2rem; }
            .concept-description-container, .main-content-area { padding: 1rem; }
            canvas { min-height: 200px; max-height: 350px; }
            .btn-primary, .btn-secondary, .btn-danger { padding: 0.6rem 1.2rem; font-size: 0.85rem; }
            .stats-value { font-size: 1.5rem; }
            .stats-label { font-size: 0.75rem; }
            .resource-bar { height: 16px; }
            .context-display { height: 80px; font-size: 0.75rem; padding: 0.5rem; }
            .tab-buttons { gap: 0.25rem; margin-bottom: 1rem; }
            .tab-button { font-size: 0.8rem; padding: 0.6rem 0.9rem; }
            .tab-content { padding: 1rem; }
            .fieldset-card { padding: 0.8rem; }
            .legend-title { font-size: 0.9rem; margin-bottom: 0.6rem; }
            .radio-grid div { padding: 0.4rem 0.6rem; }
            .radio-grid label { font-size: 0.85rem; }
            .slider-custom { height: 8px; }
            .slider-custom::-webkit-slider-thumb, .slider-custom::-moz-range-thumb { width: 18px; height: 18px; margin-top: -5px; }
            .accordion-summary, .accordion-content { font-size: 0.85rem; padding: 0.8rem 1rem; }
            .accordion-summary::after { font-size: 1.1em; margin-left: 0.6rem; }
            .concept-description { font-size: 0.8rem; margin-top: 0.6rem; }
        }

        /* Password Lock Styles */
        #password-lock {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: linear-gradient(135deg, #0F172A 0%, #020617 100%);
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            z-index: 1000;
        }
        #password-form {
            background-color: #1E293B;
            padding: 3rem;
            border-radius: 1.5rem;
            box-shadow: 0 25px 50px -12px rgba(0, 0, 0, 0.8);
            text-align: center;
            max-width: 400px;
            width: 90%;
            border: 1px solid #334155;
        }
        #password-form h2 {
            font-size: 2rem;
            font-weight: 700;
            color: #2DD4BF;
            margin-bottom: 1.5rem;
        }
        #password-input {
            width: 100%;
            padding: 1rem;
            margin-bottom: 1.5rem;
            border-radius: 0.75rem;
            border: 2px solid #475569;
            background-color: #0D1117;
            color: #E2E8F0;
            font-size: 1.2rem;
            text-align: center;
            box-shadow: inset 0 0 8px rgba(0, 0, 0, 0.3);
            outline: none;
        }
        #password-input:focus {
            border-color: #2DD4BF;
        }
        #password-submit {
            width: 100%;
            padding: 1rem;
            border-radius: 0.75rem;
            background-color: #6366F1;
            color: white;
            font-weight: 600;
            font-size: 1.1rem;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.4);
            border: none;
        }
        #password-submit:hover {
            background-color: #4F46E5;
            transform: translateY(-3px);
            box-shadow: 0 8px 15px rgba(0, 0, 0, 0.5);
        }
        #password-error {
            color: #EF4444;
            margin-top: 1rem;
            font-size: 0.9rem;
            font-weight: 500;
        }
    </style>
</head>
<body>
    <div id="password-lock">
        <div id="password-form">
            <h2>Enter Password</h2>
            <input type="password" id="password-input" placeholder="Password">
            <button id="password-submit">Unlock Simulator</button>
            <p id="password-error" class="hidden">Incorrect password. Please try again.</p>
        </div>
    </div>

    <div id="app">
        <header>
            <h1>HE's LLM Learning App</h1>
            <div class="max-w-3xl mx-auto text-center text-gray-400 text-base">
                <p class="mb-2">Visualize how Large Language Models process information, whether running locally on your device or via a cloud service.</p>
                <p>Observe resource usage, customize parameters, and explore key concepts in LLM deployment.</p>
            </div>
        </header>

        <section class="p-6 md:p-8 lg:p-10 bg-gray-800 border-t border-gray-700">
            <div class="flex flex-col items-center justify-center mb-6">
                <h3 class="text-lg font-semibold text-gray-100 mb-3">Inference Mode</h3>
                <div class="toggle-button-group">
                    <button id="modeLocalBtn" class="toggle-button active" data-mode="local">Local</button>
                    <button id="modeCloudBtn" class="toggle-button" data-mode="cloud">Cloud</button>
                </div>
            </div>

            <div class="tab-buttons">
                <button class="tab-button" data-tab="recommendations">LLM Recommendations</button>
                <button class="tab-button" data-tab="model-details">Model Details</button>
                <button class="tab-button" data-tab="companies">Companies & AI</button>
                <button class="tab-button" data-tab="people">Top People</button>
                <button class="tab-button" data-tab="frameworks-finetuning">Frameworks & Fine-tuning</button>
                <button class="tab-button" data-tab="enterprise-deployment">Enterprise Deployment</button>
                <button class="tab-button" data-tab="settings">Simulation Settings</button>
                <button class="tab-button active" data-tab="simulation">Simulation</button>
            </div>

            <div id="simulation-tab" class="tab-content active">
                <h3 class="text-2xl font-bold text-teal-400 mb-6 text-center">Simulation Overview & Controls</h3>
                
                <div class="concept-description-container">
                    <div class="bg-gray-800 rounded-2xl p-6 shadow-lg border border-gray-700">
                        <h2 class="text-xl font-semibold text-gray-100 mb-4">Concept: Visualizing LLM Inference</h2>
                        <p class="text-gray-300 mb-3">This simulator helps you understand how Large Language Models process information, whether they are running directly on your computer (Local) or through a service on the internet (Cloud).</p>
                        <ul class="list-disc list-inside ml-4 text-gray-300 text-sm">
                            <li><span class="text-green-400 font-semibold">Prompt Fragments (Green Rects):</span> These represent the pieces of your input (like text, code, or images) as they are sent for processing.</li>
                            <li><span class="text-purple-400 font-semibold">Engine Core (Central Circles):</span> This is the heart of the LLM, where the actual computation and "thinking" happen.</li>
                            <li><span class="text-yellow-300 font-semibold">Sparks:</span> These symbolize the activation of knowledge and parameters within the LLM during processing.</li>
                            <li><span class="text-pink-400 font-semibold">Output Tokens (Pink Rects):</span> These are the individual words or parts of the response that the LLM generates back to you.</li>
                        </ul>
                        <p id="modelConceptDescription" class="concept-description mt-3"></p>
                        <p id="inputTypeConceptDescription" class="concept-description"></p>
                        <p id="quantizationConceptDescription" class="concept-description"></p>
                        <p id="contextWindowConceptDescription" class="concept-description"></p>
                        <p id="presetDescription" class="concept-description"></p>
                    </div>
                </div>

                <div class="controls-stats-grid mt-6"> 
                    <div class="bg-gray-800 rounded-2xl p-4 shadow-lg border border-gray-700 flex flex-col items-center justify-center">
                        <h3 class="text-lg font-semibold text-gray-100 mb-3">Simulation Controls</h3>
                        <div class="flex flex-wrap justify-center gap-4">
                            <button id="submitPromptButton" class="btn-primary">
                                Submit Prompt
                            </button>
                            <button id="pauseButton" class="btn-secondary">
                                Pause Sim
                            </button>
                            <button id="resetButton" class="btn-danger">
                                Reset Sim
                            </button>
                        </div>
                    </div>

                    <div class="bg-gray-800 rounded-2xl p-6 shadow-lg border border-gray-700">
                        <h2 class="text-xl font-semibold text-gray-100 mb-4 text-center">Resource Utilization</h2>
                        <div class="mb-5">
                            <div class="resource-label text-gray-400 mb-1">CPU Usage</div>
                            <div class="resource-bar bg-gray-700 rounded-full h-6 overflow-hidden">
                                <div id="cpuBarVal" class="h-full bg-yellow-500 rounded-full transition-all duration-300 ease-out" style="width: 10%;"></div>
                            </div>
                        </div>
                        <div>
                            <div class="resource-label text-gray-400 mb-1">RAM Usage</div>
                            <div class="resource-bar bg-gray-700 rounded-full h-6 overflow-hidden">
                                <div id="ramBarVal" class="h-full bg-blue-500 rounded-full transition-all duration-300 ease-out" style="width: 20%;"></div>
                            </div>
                        </div>
                    </div>

                    <div class="bg-gray-800 rounded-2xl p-6 shadow-lg border border-gray-700">
                        <h2 class="text-xl font-semibold text-gray-100 mb-4 text-center">Simulation Statistics</h2>
                        <div class="grid grid-cols-1 sm:grid-cols-2 gap-4">
                            <div class="stats-card p-5">
                                <div id="promptCount" class="stats-value">0</div>
                                <div class="stats-label">Prompts Sent</div>
                            </div>
                            <div class="stats-card p-5">
                                <div id="tokensProcessedCount" class="stats-value">0</div>
                                <div class="stats-label">Fragments Processed</div>
                            </div>
                            <div class="stats-card p-5 sm:col-span-2">
                                <div id="responsesGeneratedCount" class="stats-value">0</div>
                                <div class="stats-label">Responses Generated</div>
                            </div>
                        </div>
                    </div>
                </div>

                <section class="flex flex-col gap-6 mt-6"> 
                    <div class="bg-gray-800 rounded-2xl p-4 shadow-lg border border-gray-700">
                        <h2 class="text-xl font-semibold text-gray-100 mb-4">Simulation Visualizer</h2>
                        <canvas id="llmInfCanvas" class="w-full rounded-xl border border-gray-600 shadow-inner"></canvas>
                        <div id="contextDisplay" class="context-display mt-4"></div>
                    </div>
                </section>
            </div>

            <div id="settings-tab" class="tab-content hidden">
                <div id="settings-local-content">
                    <h3 class="text-2xl font-bold text-teal-400 mb-6 text-center">Local Simulation Settings</h3>
                    <p id="presetDescription" class="concept-description text-center text-gray-400 mb-6"></p>

                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                        <fieldset class="fieldset-card">
                            <legend class="legend-title">Local Simulation Presets</legend>
                            <div id="presetRadios" class="radio-grid"></div>
                        </fieldset>

                        <fieldset class="fieldset-card">
                            <legend class="legend-title">Select Model Type</legend>
                            <div class="radio-grid">
                                <div><input type="radio" id="modelSmall" name="modelType" value="small" class="form-radio text-teal-500"><label for="modelSmall">Small</label></div>
                                <div><input type="radio" id="modelMedium" name="modelType" value="medium" class="form-radio text-teal-500"><label for="modelMedium">Medium</label></div>
                                <div><input type="radio" id="modelLarge" name="modelType" value="large" class="form-radio text-teal-500"><label for="modelLarge">Large</label></div>
                            </div>
                            <p id="modelConceptDescription" class="concept-description"></p>
                        </fieldset>

                        <fieldset class="fieldset-card">
                            <legend class="legend-title">Select Input Type</legend>
                            <div class="radio-grid">
                                <div><input type="radio" id="inputTypeText" name="inputType" value="text" class="form-radio text-teal-500"><label for="inputTypeText">Text</label></div>
                                <div><input type="radio" id="inputTypeCode" name="inputType" value="code" class="form-radio text-teal-500"><label for="inputTypeCode">Code</label></div>
                                <div><input type="radio" id="inputTypeImage" name="inputType" value="image" class="form-radio text-teal-500"><label for="inputTypeImage">Image</label></div>
                                <div><input type="radio" id="inputTypeAudio" name="inputType" value="audio" class="form-radio text-teal-500"><label for="inputTypeAudio">Audio</label></div>
                                <div><input type="radio" id="inputTypeVideo" name="inputType" value="video" class="form-radio text-teal-500"><label for="inputTypeVideo">Video</label></div>
                            </div>
                            <p id="inputTypeConceptDescription" class="concept-description"></p>
                        </fieldset>

                        <fieldset class="fieldset-card">
                            <legend class="legend-title">Quantization Level</legend>
                            <div class="radio-grid">
                                <div><input type="radio" id="quantizationFP32" name="quantizationType" value="fp32" class="form-radio text-teal-500"><label for="quantizationFP32">FP32</label></div>
                                <div><input type="radio" id="quantizationFP16" name="quantizationType" value="fp16" class="form-radio text-teal-500"><label for="quantizationFP16">FP16</label></div>
                                <div><input type="radio" id="quantizationINT8" name="quantizationType" value="int8" class="form-radio text-teal-500"><label for="quantizationINT8">INT8</label></div>
                                <div><input type="radio" id="quantizationINT4" name="quantizationType" value="int4" class="form-radio text-teal-500"><label for="quantizationINT4">INT4</label></div>
                            </div>
                            <p id="quantizationConceptDescription" class="concept-description"></p>
                        </fieldset>

                        <div class="fieldset-card md:col-span-2">
                            <div class="context-label text-center text-gray-100 mb-3">Local Context Window Size: <span id="contextWindowValue" class="text-teal-300 font-bold">2048</span> tokens</div>
                            <input type="range" id="contextWindowSlider" min="512" max="8192" value="2048" step="512" class="w-full slider-custom">
                            <p id="contextWindowConceptDescription" class="concept-description text-center"></p>
                        </div>

                        <div class="fieldset-card md:col-span-2">
                            <div class="speed-label text-center text-gray-100 mb-3">Local Simulation Speed: <span id="simulationSpeedValue" class="text-teal-300 font-bold">1.0x</span></div>
                            <input type="range" id="simulationSpeedSlider" min="0.2" max="2.0" value="1.0" step="0.2" class="w-full slider-custom">
                        </div>
                    </div>
                </div>

                <div id="settings-cloud-content" class="hidden">
                    <h3 class="text-2xl font-bold text-teal-400 mb-6 text-center">Cloud Inference Settings (Simulated)</h3>
                    <p class="text-base text-gray-400 mb-6 text-center">When using cloud LLMs, local resource usage is minimal, primarily for network communication. Settings here simulate cloud-side parameters.</p>

                    <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                        <fieldset class="fieldset-card">
                            <legend class="legend-title">Cloud Model Tier</legend>
                            <div class="radio-grid">
                                <div><input type="radio" id="cloudTierSmall" name="cloudModelTier" value="small" checked class="form-radio text-teal-500"><label for="cloudTierSmall">Small (e.g., GPT-3.5)</label></div>
                                <div><input type="radio" id="cloudTierMedium" name="cloudModelTier" value="medium" class="form-radio text-teal-500"><label for="cloudTierMedium">Medium (e.g., Gemini Pro)</label></div>
                                <div><input type="radio" id="cloudTierLarge" name="cloudModelTier" value="large" class="form-radio text-teal-500"><label for="cloudTierLarge">Large (e.g., GPT-4o)</label></div>
                            </div>
                            <p class="concept-description mt-2">Simulates the computational power of the cloud model you're accessing.</p>
                        </fieldset>

                        <fieldset class="fieldset-card">
                            <legend class="legend-title">Network Latency (Simulated)</legend>
                            <div class="radio-grid">
                                <div><input type="radio" id="latencyLow" name="networkLatency" value="low" checked class="form-radio text-teal-500"><label for="latencyLow">Low (Fast Internet)</label></div>
                                <div><input type="radio" id="latencyMedium" name="networkLatency" value="medium" class="form-radio text-teal-500"><label for="latencyMedium">Medium (Average Internet)</label></div>
                                <div><input type="radio" id="latencyHigh" name="networkLatency" value="high" class="form-radio text-teal-500"><label for="latencyHigh">High (Slow Internet)</label></div>
                            </div>
                            <p class="concept-description mt-2">Affects perceived response speed due to network conditions.</p>
                        </fieldset>

                        <div class="fieldset-card md:col-span-2">
                            <div class="context-label text-center text-gray-100 mb-3">Cloud Context Window (API Limit): <span id="cloudContextWindowValue" class="text-teal-300 font-bold">8192</span> tokens</div>
                            <input type="range" id="cloudContextWindowSlider" min="1024" max="128000" value="8192" step="1024" class="w-full slider-custom">
                            <p class="concept-description mt-2 text-center">Represents the maximum context window supported by the cloud API.</p>
                        </div>
                    </div>
                </div>
            </div>

            <div id="recommendations-tab" class="tab-content hidden">
                <div id="recommendations-local-content">
                    <h3 class="text-2xl font-bold text-teal-400 mb-4 text-center">Local LLM Recommendations for Apple Silicon (MacBook M1/M2/M3)</h3>
                    <p class="text-base text-gray-400 mb-6 text-center">Running LLMs locally on your Apple Silicon Mac leverages its Neural Engine and Unified Memory for efficient inference. Tools like Ollama, LM Studio, Jan.ai, and GPT4All simplify downloading and running these models on your own machine.</p>
                    <div class="accordion-group">
                        <details class="accordion-item">
                            <summary class="accordion-summary">Small Models (e.g., Phi-3 Mini, Mistral 7B, Llama 3 8B variants)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Best for Local Use:</strong> Ideal for quick responses, summarization, and simpler creative tasks directly on your Mac.</li>
                                    <li><strong>Local Hardware:</strong> Typically require 8GB-16GB RAM (e.g., MacBook Air M1/M2/M3 with 8GB/16GB). Quantized versions (Q4, Q5) run very well.</li>
                                    <li><strong>Local Performance:</strong> Offer good interactive performance and fast token generation on base M-series chips.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Medium Models (e.g., Yi 34B, Llama 3 Instruct 8B, Mixtral 8x7B (quantized))</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Best for Local Use:</strong> Balanced performance and capability for more complex queries, coding assistance, and detailed content generation on your local machine.</li>
                                    <li><strong>Local Hardware:</strong> Often require 16GB-32GB+ RAM (e.g., MacBook Pro M1/M2/M3 series with 16GB/32GB/36GB). Heavily reliant on quantization to fit and run smoothly.</li>
                                    <li><strong>Local Performance:</strong> Provide a good trade-off between speed and output quality for local interactive use.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Large Models (e.g., Llama 3 70B (quantized))</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Best for Local Use:</strong> Capable of highly coherent and complex responses for demanding local tasks, but will be slower.</li>
                                    <li><strong>Local Hardware:</strong> Demand substantial RAM, typically 32GB-64GB+ (e.g., Mac Studio, high-end MacBook Pro M-series Max/Ultra with 48GB/64GB/96GB/128GB). Almost exclusively run using 4-bit or lower quantization (Q2, Q3, Q4) for local feasibility.</li>
                                    <li><strong>Local Performance:</strong> Slower token generation locally, but enables access to powerful models without cloud dependence.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Key Tools for Local Deployment</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Ollama:</strong> Simple command-line tool to pull and run a wide variety of open-source models locally. Excellent for developers and quick experimentation on your Mac.</li>
                                    <li><strong>LM Studio:</strong> User-friendly GUI for discovering, downloading, configuring, and chatting with models locally. Great for exploring different models on your machine.</li>
                                    <li><strong>Jan.ai:</strong> Open-source desktop application, similar to LM Studio, for running various LLMs locally with a focus on privacy and customizability.</li>
                                    <li><strong>GPT4All:</strong> Focuses on privacy, runs models locally with a chat interface. Good for offline use on your personal computer.</li>
                                    <li><strong>Llama.cpp:</strong> Core C++ inference engine enabling efficient local execution of Llama-based and other models. Powers many of the above tools.</li>
                                </ul>
                            </div>
                        </details>
                    </div>
                </div>
                <div id="recommendations-cloud-content" class="hidden">
                    <h3 class="text-2xl font-bold text-teal-400 mb-4 text-center">Cloud LLM Recommendations & API Providers</h3>
                    <p class="text-base text-gray-400 mb-6 text-center">Accessing LLMs via cloud APIs offers scalability, powerful models, and managed infrastructure, but involves data transfer and recurring costs.</p>
                    <div class="accordion-group">
                        <details class="accordion-item">
                            <summary class="accordion-summary">OpenAI</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Models:</strong> GPT-3.5, GPT-4, GPT-4o (multimodal), DALL-E, Whisper.</li>
                                    <li><strong>Origin:</strong> San Francisco, California, USA. Incorporated in December 2015.</li>
                                    <li><strong>Best for:</strong> General-purpose AI, advanced reasoning, creative content, image generation, speech-to-text. Widely adopted, strong ecosystem.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Anthropic</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Models:</strong> Claude (Opus, Sonnet, Haiku).</li>
                                    <li><strong>Origin:</strong> San Francisco, California, USA. Incorporated in 2021.</li>
                                    <li><strong>Best for:</strong> Enterprise applications, safety-focused AI, long context windows, complex reasoning, legal and medical text processing.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Google Cloud (Vertex AI)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Models:</strong> Gemini (Pro, Ultra, Flash), Imagen, Codey, Chirp.</li>
                                    <li><strong>Origin:</strong> Mountain View, California, USA. Google was founded in 1998.</li>
                                    <li><strong>Best for:</b> Multimodal applications, enterprise solutions, integrated with Google Cloud ecosystem, strong for data analytics and code generation.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Microsoft Azure AI</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Models:</strong> Azure OpenAI Service (GPT-3.5, GPT-4), Phi-3, Llama 2.</li>
                                    <li><strong>Origin:</strong> Redmond, Washington, USA. Microsoft was founded in 1975.</li>
                                    <li><strong>Best for:</strong> Enterprises already on Azure, strong security and compliance features, integration with Microsoft services.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Cohere</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Models:</strong> Command (R, R+), Embed, Rerank.</li>
                                    <li><strong>Origin:</strong> Toronto, Ontario, Canada. Incorporated in 2019.</li>
                                    <li><strong>Best for:</b> Enterprise search, RAG (Retrieval Augmented Generation), multilingual applications, semantic search, and robust conversational AI.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Mistral AI</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Models:</strong> Mistral Large, Mistral Small, Mixtral 8x7B, Embed.</li>
                                    <li><strong>Origin:</strong> Paris, France. Incorporated in April 2023.</li>
                                    <li><strong>Best for:</strong> Efficient and powerful models, strong performance for their size, good for cost-sensitive applications, growing enterprise focus.</li>
                                </ul>
                            </div>
                        </details>
                    </div>
                </div>
            </div>

            <div id="model-details-tab" class="tab-content hidden">
                <div id="model-details-local-content">
                    <h3 class="text-2xl font-bold text-teal-400 mb-4 text-center">Leading Open-Source LLM Models for Local Deployment</h3>
                    <p class="text-base text-gray-400 mb-6 text-center">Details on prominent open-source LLMs suitable for running on your local hardware.</p>
                    <div class="accordion-group">
                        <details class="accordion-item">
                            <summary class="accordion-summary">Llama 3 (Meta) - Excellent for Local</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Purpose:</strong> General-purpose text generation, coding, reasoning.</li>
                                    <li><strong>Input Modalities:</strong> 📄 Text-based.</li>
                                    <li><strong>Origin:</strong> Developed by Meta AI, primarily out of Menlo Park, California, USA.</li>
                                    <li><strong>Key Features for Local Use:</strong> State-of-the-art performance, improved instruction following. 8B and 70B parameter versions are widely available in quantized formats (GGUF) ideal for local running.</li>
                                    <li><strong>Local Use:</strong>
                                        <ul>
                                            <li><strong>Llama 3 8B:</strong> Great for M-series Macs with 16GB RAM (even 8GB with heavy quantization). Offers fast local inference.</li>
                                            <li><strong>Llama 3 70B:</strong> Requires 32GB-64GB+ RAM locally, typically with Q4/Q5 GGUF quantization. Powerful for local creative and analytical tasks.</li>
                                        </ul>
                                    </li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Mistral 7B & Mixtral 8x7B (Mistral AI) - Optimized for Local</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> Developed by Mistral AI, a French company based in Paris, France.</li>
                                    <li><strong>Mistral 7B:</strong>
                                        <ul>
                                            <li><strong>Purpose:</strong> Highly efficient and powerful for its size.</li>
                                            <li><strong>Key Features for Local Use:</strong> GQA and SWA for fast local inference and good context handling. Outperforms many larger models locally.</li>
                                            <li><strong>Local Use:</b> Excellent for M-series Macs with 8GB-16GB RAM. Very popular for local chatbots and coding assistance.</li>
                                        </ul>
                                    </li>
                                    <li><strong>Mixtral 8x7B (SMoE):</b>
                                        <ul>
                                            <li><strong>Purpose:</strong> Top-tier open-source performance with efficient inference due to sparse MoE architecture.</li>
                                            <li><strong>Key Features for Local Use:</b> Selects expert networks per token, reducing active parameter count for local runs. Quantized GGUF versions are key.</li>
                                            <li><strong>Local Use:</strong> Requires 32GB-48GB+ RAM locally (with quantization). Provides exceptional quality for local generation and reasoning.</li>
                                        </ul>
                                    </li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Phi-3 (Microsoft) - Strong Small Local Models</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Purpose:</strong> Small, high-quality models optimized for personal devices. Mini (3.8B), Small (7B), Medium (14B).</li>
                                    <li><strong>Input Modalities:</strong> 📄 Text. Phi-3-vision adds 🖼️ image input, feasible locally.</li>
                                    <li><strong>Origin:</strong> Developed by Microsoft, with AI research centers globally, including Redmond, Washington, USA.</li>
                                    <li><strong>Key Features for Local Use:</strong> Trained on "textbook-quality" data, surprisingly capable for their small local footprint. Resource-efficient.</li>
                                    <li><strong>Local Use:</strong>
                                        <ul>
                                            <li><strong>Phi-3-mini (3.8B):</strong> Runs well even on 8GB RAM Macs, excellent for on-device AI.</li>
                                            <li><strong>Phi-3-small (7B) & medium (14B):</b> Good for 16GB+ RAM systems, offering more power locally.</li>
                                        </ul>
                                    </li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Gemma (Google) - Good for Local Experimentation</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Purpose:</strong> Lightweight open models (2B, 7B, 9B). CodeGemma for code.</li>
                                    <li><strong>Origin:</strong> Developed by Google DeepMind, with primary research in London, UK, and Mountain View, California, USA.</li>
                                    <li><strong>Key Features for Local Use:</strong> Built with Gemini tech, designed for responsible AI. Good for trying Google's model architecture locally.</li>
                                    <li><strong>Local Use:</strong> Gemma 2B and 7B are well-suited for local inference on Apple Silicon with 8GB-16GB RAM. Good balance for local tasks.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Command R / R+ (Cohere) - Primarily API, Local for Research</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Purpose:</strong> Enterprise-focused, multilingual, RAG-optimized. Command R (35B), Command R+ (104B).</li>
                                    <li><strong>Origin:</strong> Developed by Cohere, a Canadian company based in Toronto, Ontario, Canada.</li>
                                    <li><strong>Key Features:</strong> Long-context, tool use.</li>
                                    <li><strong>Local Use:</strong> Weights are available for research, but these are large models. Command R (35B) might run locally on very high-RAM systems (64GB++) with aggressive quantization. R+ (104B) is extremely demanding for local setups. <span class="note">Typically accessed via API for enterprise use rather than direct local deployment by end-users.</span></li>
                                </ul>
                            </div>
                        </details>
                    </div>
                </div>
                <div id="model-details-cloud-content" class="hidden">
                    <h3 class="text-2xl font-bold text-teal-400 mb-4 text-center">Leading Cloud API LLM Models</h3>
                    <p class="text-base text-gray-400 mb-6 text-center">Details on prominent LLMs accessible via cloud APIs, offering high performance and scalability.</p>
                    <div class="accordion-group">
                        <details class="accordion-item">
                            <summary class="accordion-summary">GPT-4o (OpenAI)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Purpose:</strong> Flagship multimodal model, highly capable across text, audio, and vision.</li>
                                    <li><strong>Input Modalities:</strong> 📄 Text, 🖼️ Image, 🎧 Audio, 📹 Video.</li>
                                    <li><strong>Origin:</strong> Developed by OpenAI, based in San Francisco, California, USA.</li>
                                    <li><strong>Key Features:</strong> State-of-the-art performance, fast response times for multimodal inputs, cost-effective compared to previous GPT-4 versions.</li>
                                    <li><strong>Use Case:</strong> Advanced chatbots, content creation, data analysis, multimodal understanding, real-time voice interaction.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Claude 3 (Anthropic)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Purpose:</strong> Family of models (Opus, Sonnet, Haiku) balancing intelligence, speed, and cost. Designed for safety and helpfulness.</li>
                                    <li><strong>Input Modalities:</b> 📄 Text, 🖼️ Image.</li>
                                    <li><strong>Origin:</strong> Developed by Anthropic, based in San Francisco, California, USA.</li>
                                    <li><strong>Key Features:</strong> Strong reasoning, long context windows (up to 200K tokens), robust for enterprise applications, excels in complex tasks requiring deep understanding.</li>
                                    <li><strong>Use Case:</strong> Legal document review, research assistance, customer service automation, code generation, creative writing.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Gemini 1.5 Pro (Google)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Purpose:</strong> Multimodal model with a massive context window for complex reasoning across modalities.</li>
                                    <li><strong>Input Modalities:</strong> 📄 Text, 🖼️ Image, 🎧 Audio, 📹 Video.</li>
                                    <li><strong>Origin:</strong> Developed by Google DeepMind, with primary research in London, UK, and Mountain View, California, USA.</li>
                                    <li><strong>Key Features:</strong> Industry-leading 1 million token context window, multimodal reasoning, strong performance on code and long documents.</li>
                                    <li><strong>Use Case:</strong> Analyzing entire codebases, summarizing long videos, deep document analysis, complex data extraction from multiple sources.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Command R+ (Cohere)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Purpose:</b> Enterprise-grade, scalable LLM optimized for RAG (Retrieval Augmented Generation) and tool use.</li>
                                    <li><strong>Input Modalities:</strong> 📄 Text.</li>
                                    <li><strong>Origin:</strong> Developed by Cohere, a Canadian company based in Toronto, Ontario, Canada.</li>
                                    <li><strong>Key Features:</strong> Strong RAG capabilities, multilingual support, high accuracy for enterprise applications, designed for seamless integration with business systems.</li>
                                    <li><strong>Use Case:</strong> Enterprise search, intelligent assistants for employees, automated reporting, data synthesis from private knowledge bases.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Mixtral 8x7B (Mistral AI)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Purpose:</strong> High-quality, efficient Sparse Mixture of Experts (SMoE) model.</li>
                                    <li><strong>Input Modalities:</strong> 📄 Text.</li>
                                    <li><strong>Origin:</strong> Developed by Mistral AI, a French company based in Paris, France.</li>
                                    <li><strong>Key Features:</strong> Strong performance for its size, efficient inference due to activating only a subset of experts per token, good for general-purpose tasks and coding.</li>
                                    <li><strong>Use Case:</b> Cost-effective API usage, chatbots, summarization, code generation, when a balance of performance and efficiency is needed.</li>
                                </ul>
                            </div>
                        </details>
                    </div>
                </div>
            </div>

            <div id="companies-tab" class="tab-content hidden">
                <div id="companies-local-content">
                    <h3 class="text-2xl font-bold text-teal-400 mb-4 text-center">Companies Advancing Local AI & On-Device LLMs</h3>
                    <p class="text-base text-gray-400 mb-6 text-center">These companies are at the forefront of enabling powerful AI to run directly on user devices.</p>
                    <div class="accordion-group">
                        <details class="accordion-item">
                            <summary class="accordion-summary">Apple</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Headquarters:</strong> Cupertino, California, USA.</li>
                                    <li><strong>Contribution:</strong> Apple Silicon (M-series chips) with Unified Memory Architecture and Neural Engine significantly accelerates on-device LLM inference. Core ML framework for deploying models.</li>
                                    <li><strong>Impact:</strong> Enables efficient local execution of models like Llama, Mistral, and Phi directly on Macs, iPhones, and iPads.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Meta</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Headquarters:</strong> Menlo Park, California, USA. Incorporated in 2004 (as Facebook).</li>
                                    <li><strong>Contribution:</strong> Developed and open-sourced the Llama series of models (Llama 2, Llama 3), which are highly optimized for local and on-device deployment due to their efficient architecture and widespread community support for quantization.</li>
                                    <li><strong>Impact:</strong> Democratized access to powerful LLMs, fostering a vast ecosystem of local AI applications.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Microsoft</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Headquarters:</strong> Redmond, Washington, USA. Founded in Albuquerque, New Mexico in 1975.</li>
                                    <li><strong>Contribution:</strong> Developed the Phi series of small, high-quality models (Phi-1.5, Phi-2, Phi-3), specifically designed to be highly capable despite their small size, making them ideal for local and edge device inference.</li>
                                    <li><strong>Impact:</b> Demonstrated that powerful AI can exist in compact forms, suitable for resource-constrained environments.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Mistral AI</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Headquarters:</strong> Paris, France. Incorporated in April 2023.</li>
                                    <li><strong>Contribution:</strong> Created Mistral 7B and Mixtral 8x7B (Sparse Mixture of Experts), models known for their exceptional performance-to-size ratio and efficient architecture, making them excellent candidates for local deployment.</li>
                                    <li><strong>Impact:</strong> Pushed the boundaries of efficient LLM design, enabling high-quality local inference with fewer resources.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Google</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Headquarters:</strong> Mountain View, California, USA. Founded at Stanford University, California in 1998.</li>
                                    <li><strong>Contribution:</strong> Developed Gemma, a family of lightweight, open models built from the same research and technology used to create the Gemini models. Designed for responsible AI and efficient deployment.</li>
                                    <li><strong>Impact:</strong> Provides a strong open-source option for developers looking to integrate Google's AI advancements into local applications.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Hugging Face</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Headquarters:</strong> New York, New York, USA and Paris, France. Incorporated in 2016.</li>
                                    <li><strong>Contribution:</strong> Not a model developer in the same vein, but a critical enabler. Provides the Transformers library, Model Hub, and Spaces, which are essential for discovering, downloading, and running open-source LLMs locally.</li>
                                    <li><strong>Impact:</strong> Centralized the open-source AI ecosystem, making local LLM experimentation and deployment vastly more accessible.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Ollama</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> USA. Founded in 2023.</li>
                                    <li><strong>Contribution:</strong> Developed an incredibly user-friendly command-line tool and API for running open-source LLMs locally on macOS, Linux, and Windows. Simplifies model downloading, setup, and interaction.</li>
                                    <li><strong>Impact:</strong> Lowered the barrier to entry for running local LLMs, making it accessible to a broader audience of users and developers.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">LM Studio / Jan.ai / GPT4All</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> Various independent developers/teams, primarily USA-based.</li>
                                    <li><strong>Contribution:</strong> Desktop applications that provide graphical user interfaces (GUIs) for downloading, managing, and interacting with local LLMs (often built on `llama.cpp`).</li>
                                    <li><strong>Impact:</strong> Made local LLM usage accessible to non-technical users, offering a chat-like experience directly on their machines.</li>
                                </ul>
                            </div>
                        </details>
                    </div>
                </div>
                <div id="companies-cloud-content" class="hidden">
                    <h3 class="text-2xl font-bold text-teal-400 mb-4 text-center">Companies Leading in Cloud AI & LLM APIs</h3>
                    <p class="text-base text-gray-400 mb-6 text-center">These companies provide powerful LLMs and AI infrastructure via cloud APIs, enabling scalable and accessible AI solutions.</p>
                    <div class="accordion-group">
                        <details class="accordion-item">
                            <summary class="accordion-summary">OpenAI</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Headquarters:</strong> San Francisco, California, USA. Incorporated in December 2015.</li>
                                    <li><strong>Focus:</strong> Pioneering large-scale AI models (GPT series, DALL-E) and making them accessible via APIs.</li>
                                    <li><strong>Impact:</strong> Democratized access to cutting-edge generative AI, driving innovation across industries.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Google</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Headquarters:</strong> Mountain View, California, USA. Founded at Stanford University, California in 1998.</li>
                                    <li><strong>Focus:</strong> Developing multimodal AI models (Gemini series) and offering a comprehensive AI platform (Vertex AI) for cloud-based ML development and deployment.</li>
                                    <li><strong>Impact:</strong> Provides integrated solutions for enterprises, leveraging Google's research in AI and cloud infrastructure.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Anthropic</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Headquarters:</strong> San Francisco, California, USA. Incorporated in 2021.</li>
                                    <li><strong>Focus:</strong> Building safe and helpful AI systems (Claude series) with an emphasis on responsible development and long context windows.</li>
                                    <li><strong>Impact:</strong> A key player in enterprise AI, offering models designed for reliability and complex business applications.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Microsoft</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Headquarters:</strong> Redmond, Washington, USA. Founded in Albuquerque, New Mexico in 1975.</li>
                                    <li><strong>Focus:</strong> Integrating AI capabilities (Azure OpenAI Service, Microsoft Copilot) into its cloud platform (Azure) and enterprise products.</li>
                                    <li><strong>Impact:</strong> Enables businesses to leverage advanced AI within their existing Microsoft ecosystems with strong security and compliance.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Amazon (AWS)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Headquarters:</strong> Seattle, Washington, USA. Founded in 1994.</li>
                                    <li><strong>Focus:</strong> Providing a wide range of AI/ML services (Amazon SageMaker, Amazon Bedrock) that allow developers to build, train, and deploy models, including foundational models from various providers.</li>
                                    <li><strong>Impact:</strong> Offers flexible and scalable AI infrastructure for diverse use cases, from custom model development to managed LLM services.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Cohere</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Headquarters:</strong> Toronto, Ontario, Canada. Incorporated in 2019.</li>
                                    <li><strong>Focus:</strong> Developing enterprise-grade LLMs (Command series) and embedding models, with a strong emphasis on RAG and multilingual capabilities.</li>
                                    <li><strong>Impact:</strong> Specializes in solutions for businesses that need robust, context-aware AI for their specific data and applications.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Mistral AI</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Headquarters:</strong> Paris, France. Incorporated in April 2023.</li>
                                    <li><strong>Models:</strong> Mistral Large, Mistral Small, Mixtral 8x7B, Embed.</li>
                                    <li><strong>Best for:</strong> Efficient and powerful models, strong performance for their size, good for cost-sensitive applications, growing enterprise focus.</li>
                                </ul>
                            </div>
                        </details>
                    </div>
                </div>
            </div>

            <div id="people-tab" class="tab-content hidden">
                <div id="people-local-content">
                    <h3 class="text-2xl font-bold text-teal-400 mb-4 text-center">Key People & Researchers in Local AI & Efficient LLMs</h3>
                    <p class="text-base text-gray-400 mb-6 text-center">These individuals have made significant contributions to making large language models run efficiently on local hardware.</p>
                    <div class="accordion-group">
                        <details class="accordion-item">
                            <summary class="accordion-summary">Georgi Gerganov</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> Bulgaria.</li>
                                    <li><strong>Key Affiliations/Discoveries:</strong> Creator of `llama.cpp` (open-source project).</li>
                                    <li><strong>Contribution:</strong> Creator of `llama.cpp`, a foundational C/C++ inference engine that enables efficient, quantized execution of LLMs on consumer hardware (including Apple Silicon).</li>
                                    <li><strong>Impact:</strong> Revolutionized local LLM inference, making models like Llama, Mistral, and many others runnable on CPUs and integrated GPUs, powering tools like LM Studio and Ollama.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Tim Dettmers</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> Germany.</li>
                                    <li><strong>Key Affiliations/Discoveries:</strong> Assistant Professor at the University of Washington (Seattle, Washington, USA). Known for work on quantization (e.g., QLoRA) during his PhD at the University of Amsterdam.</li>
                                    <li><strong>Contribution:</strong> Prominent researcher in quantization and efficient deep learning. His work on 8-bit and 4-bit quantization (e.g., QLoRA) is crucial for fitting large models into limited memory.</li>
                                    <li><strong>Impact:</b> Directly enabled the ability to run massive LLMs on consumer GPUs and even CPUs by drastically reducing memory footprint.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Yann LeCun</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> Soisy-sous-Montmorency, France.</li>
                                    <li><strong>Key Affiliations/Discoveries:</strong> Chief AI Scientist at Meta (Menlo Park, California, USA). Professor at New York University (New York, New York, USA). Pioneering work in convolutional neural networks.</li>
                                    <li><strong>Contribution:</strong> Chief AI Scientist at Meta, a strong advocate for open-source AI. His influence supports initiatives like the open release of the Llama models, which are central to local AI development.</li>
                                    <li><strong>Impact:</strong> His advocacy helps drive the availability of powerful models that can be run and fine-tuned locally.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">The Hugging Face Team (esp. core contributors to Transformers)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> Global team with primary offices in New York, New York, USA and Paris, France.</li>
                                    <li><strong>Key Affiliations/Discoveries:</strong> The company Hugging Face Inc.</li>
                                    <li><strong>Contribution:</strong> While many individuals, the collective effort behind the Hugging Face Transformers library and Model Hub has standardized and simplified the process of loading, using, and sharing LLMs, including those for local inference.</li>
                                    <li><strong>Impact:</strong> Created the de facto standard for open-source NLP and LLM development, making local experimentation vastly more accessible.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Researchers at Microsoft (e.g., for Phi models)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> Various research labs globally, including Redmond, Washington, USA.</li>
                                    <li><strong>Key Affiliations/Discoveries:</strong> Microsoft Research.</li>
                                    <li><strong>Contribution:</strong> The teams behind the Phi series of models have demonstrated that highly capable LLMs can be built with significantly fewer parameters, making them ideal for on-device and local deployment.</li>
                                    <li><strong>Impact:</b> Challenged the notion that "bigger is always better" for LLMs, opening new avenues for efficient local AI.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Researchers at Mistral AI</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> Paris, France.</li>
                                    <li><strong>Key Affiliations/Discoveries:</strong> Mistral AI (company).</li>
                                    <li><strong>Contribution:</strong> Developed highly efficient models like Mistral 7B and Mixtral 8x7B (Sparse Mixture of Experts), which are designed for strong performance with fewer computational resources, making them excellent for local inference.</li>
                                    <li><strong>Impact:</strong> Showcased innovative architectures that provide high-quality results while being more amenable to local deployment.</li>
                                </ul>
                            </div>
                        </details>
                    </div>
                </div>
                <div id="people-cloud-content" class="hidden">
                    <h3 class="text-2xl font-bold text-teal-400 mb-4 text-center">Key People & Researchers in Cloud AI & Large-Scale LLMs</h3>
                    <p class="text-base text-gray-400 mb-6 text-center">These individuals are instrumental in advancing large-scale AI models and cloud-based AI infrastructure.</p>
                    <div class="accordion-group">
                        <details class="accordion-item">
                            <summary class="accordion-summary">Sam Altman (OpenAI)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> Chicago, Illinois, USA.</li>
                                    <li><strong>Key Affiliations/Discoveries:</strong> CEO of OpenAI (San Francisco, California, USA). Previously President of Y Combinator.</li>
                                    <li><strong>Contribution:</strong> CEO of OpenAI, driving the development and widespread adoption of models like GPT-3.5, GPT-4, and GPT-4o.</li>
                                    <li><strong>Impact:</strong> Led the charge in making powerful generative AI accessible to the public and enterprises, significantly shaping the current AI landscape.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Dario Amodei (Anthropic)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> USA.</li>
                                    <li><strong>Key Affiliations/Discoveries:</strong> CEO and co-founder of Anthropic (San Francisco, California, USA). Previously VP of Research at OpenAI.</li>
                                    <li><strong>Contribution:</strong> CEO and co-founder of Anthropic, focusing on developing safe and responsible AI systems like the Claude series.</li>
                                    <li><strong>Impact:</strong> A key voice and leader in the field of AI safety and alignment, pushing for ethical considerations in large model development.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Demis Hassabis (Google DeepMind)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> London, UK.</li>
                                    <li><strong>Key Affiliations/Discoveries:</strong> CEO of Google DeepMind (London, UK, and Mountain View, California, USA). Co-founder of DeepMind. PhD in Cognitive Neuroscience from University College London.</li>
                                    <li><strong>Contribution:</strong> CEO of Google DeepMind, overseeing the development of advanced AI, including the Gemini family of models.</li>
                                    <li><strong>Impact:</strong> Leads efforts in foundational AI research and its integration into Google's products and cloud services.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Andrew Ng (DeepLearning.AI, Landing AI)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> London, UK (born); grew up in Hong Kong and Singapore.</li>
                                    <li><strong>Key Affiliations/Discoveries:</strong> Co-founder of Coursera (Mountain View, California, USA), founder of DeepLearning.AI and Landing AI. Former head of Google Brain and chief scientist at Baidu. Professor at Stanford University (Stanford, California, USA).</li>
                                    <li><strong>Contribution:</strong> Co-founder of Coursera, founder of DeepLearning.AI and Landing AI. Known for popularizing deep learning education and practical AI applications.</li>
                                    <li><strong>Impact:</strong> Educates millions on AI, driving its adoption in various industries, including cloud-based deployments.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Geoffrey Hinton (University of Toronto, Google)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> Wimbledon, London, UK.</li>
                                    <li><strong>Key Affiliations/Discoveries:</strong> Professor Emeritus at the University of Toronto (Toronto, Ontario, Canada). Former Google Brain researcher. Often referred to as the "Godfather of AI" for his foundational work in neural networks.</li>
                                    <li><strong>Contribution:</strong> "Godfather of AI," pioneering work in neural networks and deep learning.</li>
                                    <li><strong>Impact:</strong> His foundational research is critical to the development of all modern LLMs, whether run locally or in the cloud.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Oriol Vinyals (Google DeepMind)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> Spain.</li>
                                    <li><strong>Key Affiliations/Discoveries:</strong> Research Scientist at Google DeepMind (London, UK, and Mountain View, California, USA). PhD from Carnegie Mellon University.</li>
                                    <li><strong>Contribution:</strong> Prominent researcher at Google DeepMind, involved in significant advancements in large language models and reinforcement learning.</li>
                                    <li><strong>Impact:</strong> Contributes to the core research that powers many of Google's cloud-based AI capabilities.</li>
                                </ul>
                            </div>
                        </details>
                    </div>
                </div>
            </div>

            <div id="frameworks-finetuning-tab" class="tab-content hidden">
                <div id="frameworks-finetuning-local-content">
                    <h3 class="text-2xl font-bold text-teal-400 mb-4 text-center">Frameworks & Techniques for Local LLM Fine-tuning & Efficiency</h3>
                    <p class="text-base text-gray-400 mb-6 text-center">These tools and methods are crucial for optimizing and customizing LLMs for local execution.</p>
                    <div class="accordion-group">
                        <details class="accordion-item">
                            <summary class="accordion-summary">GGUF (GGML Unified Format) - The Local Standard</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Purpose:</strong> A file format for storing LLMs and their quantized weights, specifically designed for efficient CPU/GPU inference. Successor to GGML.</li>
                                    <li><strong>Key Features:</b> Supports various quantization levels (Q4_K_M, Q5_K_M, Q8_0, etc.), enabling models to run on diverse hardware with limited memory.</li>
                                    <li><strong>Local Use:</strong> The primary format for running open-source LLMs locally via `llama.cpp`, Ollama, LM Studio, etc. Essential for fitting large models onto consumer devices.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Llama.cpp - The Core Local Inference Engine</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Purpose:</strong> A C/C++ library for efficient inference of LLMs.</li>
                                    <li><strong>Origin:</strong> Developed by Georgi Gerganov (Bulgaria).</li>
                                    <li><strong>Key Features:</strong> Highly optimized for CPU and Apple Silicon (Metal), supports various quantization schemes, and is the backbone for many local LLM applications.</li>
                                    <li><strong>Local Use:</strong> Provides the underlying performance for tools like Ollama, LM Studio, and Jan.ai, making local LLM inference fast and accessible.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Quantization - Making Models Smaller & Faster</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Purpose:</strong> Reducing the precision of model weights (e.g., from FP32 to INT8 or INT4) to decrease memory footprint and increase inference speed.</li>
                                    <li><strong>Key Types for Local Use:</strong>
                                        <ul>
                                            <li><strong>FP32 (Full Precision):</strong> Highest quality, largest size, slowest. Rarely used for local LLMs due to memory demands.</li>
                                            <li><strong>FP16 (Half Precision):</strong> Good quality, half size of FP32. Still large for many local setups.</li>
                                            <li><strong>INT8/Q8_0:</strong> Good balance of quality and efficiency. Significantly reduces memory.</li>
                                            <li><strong>INT4/Q4_K_M, Q2_K:</b> Most aggressive quantization, lowest memory footprint, fastest local inference, but with some quality degradation. Ideal for running very large models on limited RAM.</li>
                                        </ul>
                                    </li>
                                    <li><strong>Impact:</strong> Crucial for enabling large LLMs to run on consumer-grade hardware with limited RAM and integrated GPUs.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">LoRA / QLoRA - Efficient Fine-tuning</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Purpose:</strong> Parameter-Efficient Fine-Tuning (PEFT) techniques that allow fine-tuning large LLMs with minimal computational resources.</li>
                                    <li><strong>Key Features:</strong>
                                        <ul>
                                            <li><strong>LoRA (Low-Rank Adaptation):</strong> Freezes original model weights and injects small, trainable low-rank matrices into each layer.</li>
                                            <li><strong>QLoRA (Quantized LoRA):</b> Extends LoRA by quantizing the base model to 4-bit, further reducing memory requirements for fine-tuning.</li>
                                        </ul>
                                    </li>
                                    <li><strong>Local Use:</strong> Enables individuals and small teams to fine-tune powerful LLMs on their local machines (with a decent GPU) for specific tasks or datasets, without needing massive cloud compute.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">ONNX / ONNX Runtime - Cross-Platform Inference</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Purpose:</strong> Open Neural Network Exchange (ONNX) is an open format for representing machine learning models. ONNX Runtime is a high-performance inference engine for ONNX models.</li>
                                    <li><strong>Origin:</strong> ONNX was initially developed by Microsoft and Facebook (Meta).</li>
                                    <li><strong>Key Features:</strong> Enables models to be trained in one framework (e.g., PyTorch) and deployed efficiently in another environment (e.g., a C++ application on a local device). Supports various hardware accelerators.</li>
                                    <li><strong>Local Use:</strong> Provides a standardized way to deploy LLMs locally across different operating systems and hardware, ensuring consistent performance and compatibility.</li>
                                </ul>
                            </div>
                        </details>
                    </div>
                </div>
                <div id="frameworks-finetuning-cloud-content" class="hidden">
                    <h3 class="text-2xl font-bold text-teal-400 mb-4 text-center">Cloud AI Frameworks & Deployment Strategies</h3>
                    <p class="text-base text-gray-400 mb-6 text-center">These frameworks and strategies are essential for developing, deploying, and managing LLMs in cloud environments.</p>
                    <div class="accordion-group">
                        <details class="accordion-item">
                            <summary class="accordion-summary">TensorFlow & PyTorch - Core ML Frameworks</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> TensorFlow by Google (Mountain View, California, USA); PyTorch by Facebook (Meta) (Menlo Park, California, USA).</li>
                                    <li><strong>Purpose:</strong> Open-source machine learning frameworks for building and training deep learning models, including LLMs, often in cloud environments.</li>
                                    <li><strong>Key Features:</strong> Extensive libraries, large communities, support for distributed training on cloud GPUs/TPUs, flexible API for custom model architectures.</li>
                                    <li><strong>Cloud Use:</strong> Used by cloud providers and developers to train and fine-tune foundational models, and to build custom AI applications that run on cloud infrastructure.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Kubernetes & Docker - Containerization & Orchestration</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> Kubernetes was originally designed by Google (Mountain View, California, USA); Docker Inc. (San Francisco, California, USA).</li>
                                    <li><strong>Purpose:</strong> Docker for packaging applications into portable containers; Kubernetes for automating deployment, scaling, and management of containerized applications.</li>
                                    <li><strong>Key Features:</strong> Ensures consistent environments from development to production, enables efficient resource utilization, high availability, and scalability for LLM inference services.</li>
                                    <li><strong>Cloud Use:</strong> Standard tools for deploying LLM APIs and microservices on cloud platforms (e.g., Google Kubernetes Engine, Azure Kubernetes Service, Amazon EKS).</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Cloud ML Platforms (e.g., Vertex AI, Azure ML, SageMaker)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Origin:</strong> Vertex AI by Google (Mountain View, California, USA); Azure ML by Microsoft (Redmond, Washington, USA); Amazon SageMaker by Amazon (Seattle, Washington, USA).</li>
                                    <li><strong>Purpose:</strong> Managed services from cloud providers that offer end-to-end MLOps capabilities, from data preparation to model deployment and monitoring.</li>
                                    <li><strong>Key Features:</strong> Pre-built tools for LLM fine-tuning, managed inference endpoints, MLOps automation, integration with other cloud services.</li>
                                    <li><strong>Cloud Use:</strong> Simplifies the deployment and management of LLMs for enterprises, abstracting away complex infrastructure management.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Fine-tuning & Prompt Engineering</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Purpose:</strong>
                                        <ul>
                                            <li><strong>Fine-tuning:</strong> Adapting a pre-trained LLM to a specific task or dataset by further training it on new data.</li>
                                            <li><strong>Prompt Engineering:</strong> Crafting effective prompts to guide an LLM to generate desired outputs without changing its weights.</li>
                                        </ul>
                                    </li>
                                    <li><strong>Key Features:</strong> Fine-tuning enhances model performance for niche tasks; prompt engineering allows flexible, on-the-fly adaptation.</li>
                                    <li><strong>Cloud Use:</strong> Both are critical for customizing cloud LLMs. Fine-tuning often uses cloud compute (e.g., GPUs), while prompt engineering is a core interaction method for cloud APIs.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">RAG (Retrieval Augmented Generation)</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Purpose:</strong> Enhancing LLM responses by retrieving relevant information from external knowledge bases before generating a response.</li>
                                    <li><strong>Key Features:</b> Improves factual accuracy, reduces hallucinations, allows LLMs to access up-to-date or proprietary information.</li>
                                    <li><strong>Cloud Use:</strong> Widely implemented with cloud LLMs. Involves cloud-based vector databases (e.g., Pinecone, Weaviate) and orchestration frameworks (e.g., LangChain, LlamaIndex) to connect LLMs with external data sources.</li>
                                </ul>
                            </div>
                        </details>
                    </div>
                </div>
            </div>

            <div id="enterprise-deployment-tab" class="tab-content hidden">
                <div id="enterprise-deployment-local-content">
                    <h3 class="text-2xl font-bold text-teal-400 mb-4 text-center">Enterprise Local & On-Premise LLM Deployment</h3>
                    <p class="text-base text-gray-400 mb-6 text-center">For businesses, deploying LLMs locally or on-premise offers significant advantages in data privacy, cost control, and customization.</p>
                    <div class="accordion-group">
                        <details class="accordion-item">
                            <summary class="accordion-summary">Data Privacy & Security</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Benefit:</strong> Sensitive corporate data never leaves the company's controlled environment, addressing strict compliance requirements (e.g., GDPR, HIPAA).</li>
                                    <li><strong>Mechanism:</strong> LLM inference runs entirely within the enterprise's data centers or on employee devices, preventing data exfiltration to third-party cloud providers.</li>
                                    <li><strong>Use Case:</strong> Financial institutions, healthcare providers, legal firms handling confidential client information.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Cost Efficiency & Predictability</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Benefit:</strong> Eliminates recurring API costs associated with cloud LLMs, leading to predictable operational expenses after initial hardware investment.</li>
                                    <li><strong>Mechanism:</strong> Once hardware is procured and models are deployed, inference costs are primarily electricity and maintenance, not per-token usage fees.</li>
                                    <li><strong>Use Case:</b> High-volume internal applications like customer support automation, internal knowledge base querying, code generation for developers.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Customization & Fine-tuning</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Benefit:</strong> Ability to fine-tune LLMs on proprietary datasets without exposing that data to external services, creating highly specialized models.</li>
                                    <li><strong>Mechanism:</strong> Training and fine-tuning pipelines are run on internal infrastructure, allowing for deep customization of models to specific business needs and terminology.</li>
                                    <li><strong>Use Case:</b> Developing an LLM specialized in internal company policies, product documentation, or industry-specific jargon.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Low Latency & Offline Capability</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Benefit:</strong> Inference can occur with minimal network delay, and applications can function even without internet connectivity.</li>
                                    <li><strong>Mechanism:</b> Models are physically located close to or on the devices where inference is needed, reducing round-trip times.</li>
                                    <li><strong>Use Case:</strong> Edge devices in manufacturing, autonomous vehicles, field service applications, or scenarios requiring real-time responses.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Full Control & Vendor Lock-in Avoidance</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Benefit:</strong> Enterprises retain complete control over the LLM stack, from model selection to deployment and updates, avoiding reliance on single cloud providers.</li>
                                    <li><strong>Mechanism:</strong> Utilizing open-source models and frameworks allows for greater flexibility and the ability to switch models or infrastructure as needed.</li>
                                    <li><strong>Use Case:</strong> Strategic long-term AI initiatives where flexibility and control over intellectual property are paramount.</li>
                                </ul>
                            </div>
                        </details>
                    </div>
                </div>
                <div id="enterprise-deployment-cloud-content" class="hidden">
                    <h3 class="text-2xl font-bold text-teal-400 mb-4 text-center">Enterprise Cloud LLM Deployment</h3>
                    <p class="text-base text-gray-400 mb-6 text-center">For businesses, deploying LLMs in the cloud offers significant advantages in scalability, managed services, and global accessibility.</p>
                    <div class="accordion-group">
                        <details class="accordion-item">
                            <summary class="accordion-summary">Scalability & Elasticity</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Benefit:</strong> Easily scale LLM inference capacity up or down based on demand, handling fluctuating workloads without over-provisioning hardware.</li>
                                    <li><strong>Mechanism:</strong> Cloud providers offer managed services that automatically adjust resources (e.g., GPUs) to meet real-time traffic, paying only for what's used.</li>
                                    <li><strong>Use Case:</strong> High-traffic customer-facing applications, seasonal demand spikes, rapid prototyping and experimentation.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Managed Services & Reduced Operational Overhead</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Benefit:</strong> Cloud providers handle infrastructure management, patching, security, and scaling, freeing up internal teams to focus on AI development.</li>
                                    <li><strong>Mechanism:</strong> Services like Google Vertex AI, Azure Machine Learning, and Amazon SageMaker provide fully managed LLM endpoints and MLOps pipelines.</li>
                                    <li><strong>Use Case:</strong> Businesses with limited MLOps expertise, faster time-to-market for AI products, reducing total cost of ownership.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Access to State-of-the-Art Models</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Benefit:</strong> Immediate access to the latest, most powerful, and often proprietary LLMs (e.g., GPT-4o, Claude Opus, Gemini 1.5 Pro) without needing to host them.</li>
                                    <li><strong>Mechanism:</strong> Cloud providers offer these models via APIs, allowing businesses to integrate cutting-edge AI into their applications with minimal effort.</li>
                                    <li><strong>Use Case:</strong> Applications requiring the highest accuracy, most creative outputs, or multimodal capabilities that are too large or complex to run locally.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Global Reach & Low Latency</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Benefit:</strong> Deploy LLM services globally, serving users with low latency regardless of their geographical location.</li>
                                    <li><strong>Mechanism:</strong> Cloud data centers are distributed worldwide, allowing businesses to deploy their AI services closer to their end-users.</li>
                                    <li><strong>Use Case:</strong> Global customer support, international content generation, applications with a geographically dispersed user base.</li>
                                </ul>
                            </div>
                        </details>
                        <details class="accordion-item">
                            <summary class="accordion-summary">Integration with Cloud Ecosystem</summary>
                            <div class="accordion-content">
                                <ul>
                                    <li><strong>Benefit:</strong> Seamless integration with other cloud services (databases, data lakes, analytics tools, security features) for comprehensive AI solutions.</li>
                                    <li><strong>Mechanism:</strong> LLM APIs are part of broader cloud platforms, enabling easy data flow, monitoring, and security management.</li>
                                    <li><strong>Use Case:</strong> Building complex AI applications that leverage existing cloud data and infrastructure investments.</li>
                                </ul>
                            </div>
                        </details>
                    </div>
                </div>
            </div>
        </section>
    </div>

    <script>
        // Canvas setup
        const canvas = document.getElementById('llmInfCanvas');
        const ctx = canvas.getContext('2d');
        let animationFrameId;
        let isPaused = false;
        let lastFrameTime = 0;
        let deltaTime = 0; // Time elapsed since last frame in seconds

        // Simulation parameters
        let inferenceMode = 'local'; // 'local' or 'cloud'
        let currentModelType = 'small'; // For local
        let currentInputType = 'text'; // For local
        let currentQuantizationType = 'int8'; // For local
        let contextWindowSize = 2048; // For local
        let simulationSpeed = 1.0; // Multiplier for animation speed

        // Cloud-specific parameters
        let cloudModelTier = 'small';
        let networkLatency = 'low';
        let cloudContextWindow = 8192;

        // Simulation statistics
        let promptsSent = 0;
        let tokensProcessed = 0;
        let responsesGenerated = 0;
        let cpuUsage = 10; // Percentage
        let ramUsage = 20; // Percentage

        // Particle arrays
        let promptFragments = [];
        let engineCoreParticles = [];
        let sparks = [];
        let outputTokens = [];

        // UI Elements
        const submitPromptButton = document.getElementById('submitPromptButton');
        const pauseButton = document.getElementById('pauseButton');
        const resetButton = document.getElementById('resetButton');
        const cpuBarVal = document.getElementById('cpuBarVal');
        const ramBarVal = document.getElementById('ramBarVal');
        const promptCountElem = document.getElementById('promptCount');
        const tokensProcessedCountElem = document.getElementById('tokensProcessedCount');
        const responsesGeneratedCountElem = document.getElementById('responsesGeneratedCount');
        const contextWindowValueElem = document.getElementById('contextWindowValue');
        const simulationSpeedValueElem = document.getElementById('simulationSpeedValue');
        const contextWindowSlider = document.getElementById('contextWindowSlider');
        const simulationSpeedSlider = document.getElementById('simulationSpeedSlider');
        const contextDisplay = document.getElementById('contextDisplay');
        const presetRadiosContainer = document.getElementById('presetRadios');
        const cloudContextWindowValueElem = document.getElementById('cloudContextWindowValue');
        const cloudContextWindowSlider = document.getElementById('cloudContextWindowSlider');

        // Password Lock Elements
        const passwordLock = document.getElementById('password-lock');
        const passwordInput = document.getElementById('password-input');
        const passwordSubmit = document.getElementById('password-submit');
        const passwordError = document.getElementById('password-error');
        const appContainer = document.getElementById('app');

        // Inference Mode Buttons
        const modeLocalBtn = document.getElementById('modeLocalBtn');
        const modeCloudBtn = document.getElementById('modeCloudBtn');


        // Concept Description Elements
        const modelConceptDescription = document.getElementById('modelConceptDescription');
        const inputTypeConceptDescription = document.getElementById('inputTypeConceptDescription');
        const quantizationConceptDescription = document.getElementById('quantizationConceptDescription');
        const contextWindowConceptDescription = document.getElementById('contextWindowConceptDescription');
        const presetDescriptionElem = document.getElementById('presetDescription');


        // Configuration objects
        const modelConfigs = {
            small: {
                cpuMultiplier: 0.5,
                ramMultiplier: 0.7,
                processingSpeed: 1.5, // tokens per frame tick
                description: "Small models (e.g., Phi-3 Mini, Mistral 7B) are efficient, requiring less CPU and RAM for quick local responses."
            },
            medium: {
                cpuMultiplier: 1.0,
                ramMultiplier: 1.2,
                processingSpeed: 1.0,
                description: "Medium models (e.g., Llama 3 8B, Mixtral 8x7B quantized) offer a balance of capability and local resource usage."
            },
            large: {
                cpuMultiplier: 1.8,
                ramMultiplier: 2.5,
                processingSpeed: 0.5,
                description: "Large models (e.g., Llama 3 70B quantized) are powerful but demand significant local CPU and RAM, leading to slower inference."
            }
        };

        const inputTypeConfigs = {
            text: {
                complexity: 1.0, // Multiplier for processing cost
                fragmentCount: 20,
                description: "Text input is the most common and generally efficient for local LLMs."
            },
            code: {
                complexity: 1.3,
                fragmentCount: 30,
                description: "Code input requires more complex parsing and understanding, slightly increasing local processing load."
            },
            image: {
                complexity: 2.0,
                fragmentCount: 40,
                description: "Image input (multimodal) requires significant local pre-processing and higher resource usage for visual tokenization."
            },
            audio: {
                complexity: 2.5,
                fragmentCount: 50,
                description: "Audio input (multimodal) involves local speech-to-text or direct audio processing, which is resource-intensive."
            },
            video: {
                complexity: 3.0,
                fragmentCount: 60,
                description: "Video input (multimodal) is the most demanding, requiring frame-by-frame local analysis and high computational power."
            }
        };

        const quantizationConfigs = {
            fp32: {
                cpuEfficiency: 1.5, // Lower efficiency means higher CPU usage
                ramEfficiency: 1.5, // Lower efficiency means higher RAM usage
                speedMultiplier: 0.7, // Slower
                description: "FP32 (Full Precision) offers highest quality but is very resource-intensive, rarely used for local LLMs."
            },
            fp16: {
                cpuEfficiency: 1.2,
                ramEfficiency: 1.2,
                speedMultiplier: 0.9,
                description: "FP16 (Half Precision) balances quality and efficiency, but still requires substantial local RAM."
            },
            int8: {
                cpuEfficiency: 1.0,
                ramEfficiency: 1.0,
                speedMultiplier: 1.0,
                description: "INT8 (8-bit Integer) is a common quantization, offering good local performance and reduced memory."
            },
            int4: {
                cpuEfficiency: 0.8,
                ramEfficiency: 0.7,
                speedMultiplier: 1.2, // Faster
                description: "INT4 (4-bit Integer) is highly efficient, drastically reducing local RAM and boosting speed, with minor quality loss."
            }
        };

        const presetConfigs = {
            "Standard Laptop": {
                modelType: 'small',
                inputType: 'text',
                quantizationType: 'int8',
                contextWindow: 2048,
                speed: 1.0,
                description: "Simulates a typical laptop setup, optimized for general text tasks with a compact model."
            },
            "High-End Workstation": {
                modelType: 'medium',
                inputType: 'code',
                quantizationType: 'fp16',
                contextWindow: 4096,
                speed: 1.5,
                description: "Represents a powerful workstation, handling larger models and more complex inputs with higher precision."
            },
            "Edge Device (IoT)": {
                modelType: 'small',
                inputType: 'audio',
                quantizationType: 'int4',
                contextWindow: 512,
                speed: 0.8,
                description: "Mimics a resource-constrained edge device, prioritizing extreme efficiency for specific tasks."
            },
            "AI Enthusiast Rig": {
                modelType: 'large',
                inputType: 'image',
                quantizationType: 'int4',
                contextWindow: 8192,
                speed: 1.2,
                description: "Designed for users with dedicated AI hardware, pushing limits with large models and multimodal inputs."
            }
        };

        const cloudModelConfigs = {
            small: {
                cpuUsage: 5, // Minimal local CPU for API calls
                ramUsage: 10, // Minimal local RAM for data transfer
                processingSpeed: 2.0, // Fast cloud API response
                description: "Small cloud models (e.g., GPT-3.5) offer fast, cost-effective inference for general tasks."
            },
            medium: {
                cpuUsage: 8,
                ramUsage: 15,
                processingSpeed: 1.5,
                description: "Medium cloud models (e.g., Gemini Pro) balance capability and speed for more complex cloud tasks."
            },
            large: {
                cpuUsage: 12,
                ramUsage: 20,
                processingSpeed: 1.0,
                description: "Large cloud models (e.g., GPT-4o) provide cutting-edge performance but may have higher latency and cost."
            }
        };

        const networkLatencyConfigs = {
            low: {
                speedMultiplier: 1.5,
                description: "Low network latency simulates a fast internet connection, resulting in quicker cloud responses."
            },
            medium: {
                speedMultiplier: 1.0,
                description: "Medium network latency represents average internet conditions, with typical cloud response times."
            },
            high: {
                speedMultiplier: 0.5,
                description: "High network latency simulates a slow internet connection, significantly impacting cloud response times."
            }
        };

        // Message box for alerts
        function showMessageBox(message, type = 'info') {
            const messageBox = document.createElement('div');
            messageBox.className = `fixed bottom-4 right-4 p-4 rounded-lg shadow-lg text-white z-50`;
            if (type === 'error') {
                messageBox.classList.add('bg-red-600');
            } else if (type === 'success') {
                messageBox.classList.add('bg-green-600');
            } else {
                messageBox.classList.add('bg-blue-600');
            }
            messageBox.textContent = message;
            document.body.appendChild(messageBox);

            setTimeout(() => {
                messageBox.remove();
            }, 3000);
        }

        // Particle classes
        class PromptFragment {
            constructor(x, y, targetX, targetY, size, speed) {
                this.x = x;
                this.y = y;
                this.targetX = targetX;
                this.targetY = targetY;
                this.size = size;
                this.speed = speed;
                this.color = '#22c55e'; // Green
                this.reachedTarget = false;
            }

            update(dt) {
                if (this.reachedTarget) return;

                const dx = this.targetX - this.x;
                const dy = this.targetY - this.y;
                const distance = Math.sqrt(dx * dx + dy * dy);

                if (distance < this.speed * dt) {
                    this.x = this.targetX;
                    this.y = this.targetY;
                    this.reachedTarget = true;
                } else {
                    this.x += (dx / distance) * this.speed * dt;
                    this.y += (dy / distance) * this.speed * dt;
                }
            }

            draw(ctx) {
                ctx.fillStyle = this.color;
                ctx.fillRect(this.x - this.size / 2, this.y - this.size / 2, this.size, this.size);
                ctx.strokeStyle = '#16a34a'; // Darker green border
                ctx.lineWidth = 1;
                ctx.strokeRect(this.x - this.size / 2, this.y - this.size / 2, this.size, this.size);
            }
        }

        class Spark {
            constructor(x, y, radius, angle, speed) {
                this.x = x;
                this.y = y;
                this.radius = radius;
                this.angle = angle;
                this.speed = speed + (Math.random() * 50 - 25); // Add some variance
                this.life = 1.0; // 1.0 = full life, 0.0 = dead
                this.decayRate = 0.02 + Math.random() * 0.03; // Faster decay for more dynamic effect
                this.color = '#facc15'; // Yellow
            }

            update(dt) {
                this.x += Math.cos(this.angle) * this.speed * dt;
                this.y += Math.sin(this.angle) * this.speed * dt;
                this.life -= this.decayRate * dt * 60; // Scale decay by dt
            }

            draw(ctx) {
                ctx.globalAlpha = Math.max(0, this.life);
                ctx.fillStyle = this.color;
                ctx.beginPath();
                ctx.arc(this.x, this.y, this.radius * this.life, 0, Math.PI * 2);
                ctx.fill();
                ctx.globalAlpha = 1.0;
            }
        }

        class OutputToken {
            constructor(x, y, startAngle, size, speed) {
                this.x = x;
                this.y = y;
                this.angle = startAngle + (Math.random() - 0.5) * Math.PI / 4; // Spread out
                this.size = size;
                this.speed = speed;
                this.color = '#ec4899'; // Pink
                this.life = 1.0;
                this.decayRate = 0.005;
            }

            update(dt) {
                this.x += Math.cos(this.angle) * this.speed * dt;
                this.y += Math.sin(this.angle) * this.speed * dt;
                this.life -= this.decayRate * dt * 60;
            }

            draw(ctx) {
                ctx.globalAlpha = Math.max(0, this.life);
                ctx.fillStyle = this.color;
                ctx.fillRect(this.x - this.size / 2, this.y - this.size / 2, this.size, this.size);
                ctx.strokeStyle = '#be185d'; // Darker pink border
                ctx.lineWidth = 1;
                ctx.strokeRect(this.x - this.size / 2, this.y - this.size / 2, this.size, this.size);
                ctx.globalAlpha = 1.0;
            }
        }

        // Core drawing
        function drawEngineCore() {
            const centerX = canvas.width / 2;
            const centerY = canvas.height / 2;
            const baseRadius = Math.min(canvas.width, canvas.height) * 0.15;

            // Outer glow effect
            ctx.shadowBlur = 30;
            ctx.shadowColor = '#8b5cf6'; // Violet glow

            // Draw multiple concentric circles for the core layers
            for (let i = 0; i < 3; i++) {
                const radius = baseRadius - (i * 15);
                // Ensure radius is positive before drawing
                if (radius <= 0) {
                    continue;
                }
                const alpha = 0.8 - (i * 0.2);
                ctx.beginPath();
                ctx.arc(centerX, centerY, radius, 0, Math.PI * 2);
                ctx.fillStyle = `rgba(139, 92, 246, ${alpha})`; // Violet
                ctx.fill();
                ctx.strokeStyle = `rgba(124, 58, 237, ${alpha})`; // Darker violet border
                ctx.lineWidth = 2;
                ctx.stroke();
            }

            ctx.shadowBlur = 0; // Reset shadow
        }

        // Update resource gauges
        function updateResourceGauges() {
            cpuBarVal.style.width = `${cpuUsage}%`;
            ramBarVal.style.width = `${ramUsage}%`;
        }

        // Update statistics
        function updateStats() {
            promptCountElem.textContent = promptsSent;
            tokensProcessedCountElem.textContent = tokensProcessed;
            responsesGeneratedCountElem.textContent = responsesGenerated;
        }

        // Update concept descriptions
        function updateDescriptions() {
            if (inferenceMode === 'local') {
                modelConceptDescription.textContent = modelConfigs[currentModelType].description;
                inputTypeConceptDescription.textContent = inputTypeConfigs[currentInputType].description;
                quantizationConceptDescription.textContent = quantizationConfigs[currentQuantizationType].description;
                contextWindowConceptDescription.textContent = `Adjust the context window size for local memory usage. Current: ${contextWindowSize} tokens. Larger context windows require more RAM.`;
            } else { // Cloud mode
                modelConceptDescription.textContent = cloudModelConfigs[cloudModelTier].description;
                inputTypeConceptDescription.textContent = "Input type determines the complexity of data sent to the cloud LLM.";
                quantizationConceptDescription.textContent = "Quantization is handled by the cloud provider and doesn't affect local resources.";
                contextWindowConceptDescription.textContent = `Adjust the simulated cloud context window size. Current: ${cloudContextWindow} tokens. This is the maximum context the cloud API can handle.`;
            }

            // Update preset description based on selected preset
            const selectedPresetRadio = document.querySelector('input[name="presetType"]:checked');
            if (selectedPresetRadio && inferenceMode === 'local') { // Only show preset description in local mode
                presetDescriptionElem.textContent = presetConfigs[selectedPresetRadio.value].description;
            } else {
                presetDescriptionElem.textContent = ""; // Clear if no preset selected or in cloud mode
            }
        }

        // Apply a preset
        function applyPreset(presetName) {
            const preset = presetConfigs[presetName];
            if (preset) {
                // Update radio buttons
                document.querySelector(`input[name="modelType"][value="${preset.modelType}"]`).checked = true;
                document.querySelector(`input[name="inputType"][value="${preset.inputType}"]`).checked = true;
                document.querySelector(`input[name="quantizationType"][value="${preset.quantizationType}"]`).checked = true;

                // Update sliders
                contextWindowSlider.value = preset.contextWindow;
                simulationSpeedSlider.value = preset.speed;

                // Manually trigger updates for sliders
                contextWindowSlider.dispatchEvent(new Event('input'));
                simulationSpeedSlider.dispatchEvent(new Event('input'));

                // Update current simulation parameters
                currentModelType = preset.modelType;
                currentInputType = preset.inputType;
                currentQuantizationType = preset.quantizationType;
                contextWindowSize = preset.contextWindow;
                simulationSpeed = preset.speed;

                updateSimulationParameters();
                updateDescriptions();
            }
        }

        // Populate preset radios
        function populatePresetRadios() {
            presetRadiosContainer.innerHTML = ''; // Clear existing
            for (const presetName in presetConfigs) {
                const div = document.createElement('div');
                const input = document.createElement('input');
                input.type = 'radio';
                input.id = `preset${presetName.replace(/\s/g, '')}`;
                input.name = 'presetType';
                input.value = presetName;
                input.addEventListener('change', () => applyPreset(presetName));

                const label = document.createElement('label');
                label.htmlFor = input.id;
                label.textContent = presetName;

                div.appendChild(input);
                div.appendChild(label);
                presetRadiosContainer.appendChild(div);
            }
        }

        // Update simulation parameters based on selected settings
        function updateSimulationParameters() {
            if (inferenceMode === 'local') {
                const modelConfig = modelConfigs[currentModelType];
                const inputConfig = inputTypeConfigs[currentInputType];
                const quantizationConfig = quantizationConfigs[currentQuantizationType];

                // Calculate CPU and RAM usage for local mode
                let newCpuUsage = 20 * modelConfig.cpuMultiplier * inputConfig.complexity / quantizationConfig.cpuEfficiency;
                let newRamUsage = 30 * modelConfig.ramMultiplier * (contextWindowSize / 2048) / quantizationConfig.ramEfficiency;

                cpuUsage = Math.min(100, Math.max(0, newCpuUsage));
                ramUsage = Math.min(100, Math.max(0, newRamUsage));

                // Enable local settings controls
                document.querySelectorAll('#settings-local-content fieldset, #settings-local-content .context-window-container, #settings-local-content .simulation-speed-container').forEach(elem => {
                    elem.style.pointerEvents = 'auto';
                    elem.style.opacity = '1';
                });
                document.querySelectorAll('#settings-local-content input[type="radio"], #settings-local-content input[type="range"]').forEach(input => {
                    input.disabled = false;
                });

            } else { // Cloud mode
                const cloudModel = cloudModelConfigs[cloudModelTier];
                const latencyConfig = networkLatencyConfigs[networkLatency];

                // Fixed low local CPU/RAM usage for cloud mode (for API calls, data transfer)
                cpuUsage = cloudModel.cpuUsage;
                ramUsage = cloudModel.ramUsage;

                // Disable local settings controls and enable cloud settings controls
                document.querySelectorAll('#settings-local-content fieldset, #settings-local-content .context-window-container, #settings-local-content .simulation-speed-container').forEach(elem => {
                    elem.style.pointerEvents = 'none';
                    elem.style.opacity = '0.5'; // Visually indicate disabled
                });
                document.querySelectorAll('#settings-local-content input[type="radio"], #settings-local-content input[type="range"]').forEach(input => {
                    input.disabled = true;
                });

                // Update cloud context window display
                cloudContextWindowValueElem.textContent = cloudContextWindow;
            }
            updateResourceGauges();
        }

        // Function to update tab content visibility based on inference mode
        function updateTabContentVisibility() {
            const tabs = ['simulation', 'settings', 'recommendations', 'model-details', 'companies', 'people', 'frameworks-finetuning', 'enterprise-deployment'];

            tabs.forEach(tabId => {
                const localContent = document.getElementById(`${tabId}-local-content`);
                const cloudContent = document.getElementById(`${tabId}-cloud-content`);
                const mainContent = document.getElementById(`${tabId}`); // For tabs that don't have local/cloud split

                if (localContent) localContent.style.display = inferenceMode === 'local' ? 'block' : 'none';
                if (cloudContent) cloudContent.style.display = inferenceMode === 'cloud' ? 'block' : 'none';
                if (mainContent && !localContent && !cloudContent) { // If it's a tab without local/cloud split
                    mainContent.classList.remove('hidden'); // Ensure it's visible if it's the active tab
                }
            });
        }


        // Initialize simulation state
        function initSimulation() {
            // Reset all counters and arrays
            promptsSent = 0;
            tokensProcessed = 0;
            responsesGenerated = 0;
            promptFragments = [];
            sparks = [];
            outputTokens = [];
            contextDisplay.innerHTML = '';
            isPaused = false;
            pauseButton.textContent = 'Pause Sim';

            // Set default inference mode to local and update buttons
            inferenceMode = 'local';
            modeLocalBtn.classList.add('active');
            modeCloudBtn.classList.remove('active');

            // Apply default local settings (e.g., "Standard Laptop" preset)
            applyPreset("Standard Laptop");

            // Apply default cloud settings
            document.querySelector('#cloudTierSmall').checked = true;
            cloudModelTier = 'small';
            document.querySelector('#latencyLow').checked = true;
            networkLatency = 'low';
            cloudContextWindowSlider.value = 8192;
            cloudContextWindow = 8192;


            updateStats();
            updateTabContentVisibility(); // Update tab content based on default mode
            updateSimulationParameters(); // Update resource usage based on default mode
            updateDescriptions(); // Update descriptions on reset
            draw(); // Draw initial state
        }

        // Function to start a new prompt simulation
        function submitPrompt() {
            promptsSent++;
            updateStats();

            let numFragments;
            let fragmentSpeed;
            let processingDelay;

            if (inferenceMode === 'local') {
                const inputConfig = inputTypeConfigs[currentInputType];
                const modelConfig = modelConfigs[currentModelType];
                const quantizationConfig = quantizationConfigs[currentQuantizationType];

                numFragments = inputConfig.fragmentCount;
                fragmentSpeed = 150 + (modelConfig.processingSpeed * 50) * quantizationConfig.speedMultiplier;
                processingDelay = (numFragments / (modelConfig.processingSpeed * quantizationConfig.speedMultiplier)) * 10;
            } else { // Cloud mode
                const cloudModel = cloudModelConfigs[cloudModelTier];
                const latencyConfig = networkLatencyConfigs[networkLatency];

                numFragments = 10; // Fewer fragments for cloud to represent API call
                fragmentSpeed = 300; // Fast "API call" fragments
                processingDelay = (numFragments / (cloudModel.processingSpeed * latencyConfig.speedMultiplier)) * 20; // Cloud processing + network latency
            }

            const fragmentSize = 8;
            const centerX = canvas.width / 2;
            const centerY = canvas.height / 2;

            // Generate prompt fragments from random directions
            for (let i = 0; i < numFragments; i++) {
                const startX = Math.random() < 0.5 ? 0 : canvas.width;
                const startY = Math.random() * canvas.height;
                promptFragments.push(new PromptFragment(startX, startY, centerX, centerY, fragmentSize, fragmentSpeed));
            }

            // Simulate LLM response after a delay
            setTimeout(() => {
                if (!isPaused) { // Only generate if not paused
                    generateResponse();
                }
            }, processingDelay / simulationSpeed); // Scale delay by simulation speed
        }

        // Function to generate LLM response tokens
        function generateResponse() {
            responsesGenerated++;
            updateStats();

            let numOutputTokens;
            let outputTokenSpeed;

            if (inferenceMode === 'local') {
                const modelConfig = modelConfigs[currentModelType];
                const quantizationConfig = quantizationConfigs[currentQuantizationType];
                numOutputTokens = Math.floor(Math.random() * 30) + 10; // 10-40 tokens
                outputTokenSpeed = 100 + (modelConfig.processingSpeed * 30) * quantizationConfig.speedMultiplier;
            } else { // Cloud mode
                const cloudModel = cloudModelConfigs[cloudModelTier];
                const latencyConfig = networkLatencyConfigs[networkLatency];
                numOutputTokens = Math.floor(Math.random() * 20) + 5; // Fewer tokens for cloud to represent API response
                outputTokenSpeed = 150 + (cloudModel.processingSpeed * 30) * latencyConfig.speedMultiplier;
            }
            
            const outputTokenSize = 8;
            const centerX = canvas.width / 2;
            const centerY = canvas.height / 2;

            // Display a simulated response in the context window
            const simulatedResponse = generateSimulatedLLMResponse(numOutputTokens);
            const responseElement = document.createElement('p');
            responseElement.innerHTML = `<span class="prompt-frag-text">Prompt ${promptsSent}:</span> ${simulatedResponse.prompt} <span class="output-token-text">Response:</span> ${simulatedResponse.response}`;
            contextDisplay.appendChild(responseElement);
            contextDisplay.scrollTop = contextDisplay.scrollHeight; // Scroll to bottom

            // Generate output tokens moving outwards
            for (let i = 0; i < numOutputTokens; i++) {
                const angle = Math.random() * Math.PI * 2; // Random angle
                outputTokens.push(new OutputToken(centerX, centerY, angle, outputTokenSize, outputTokenSpeed));
            }
        }

        // Generate simulated LLM response text
        function generateSimulatedLLMResponse(numTokens) {
            const promptWords = ["Analyze", "Summarize", "Generate", "Explain", "Code", "Translate", "Draft", "Predict", "Describe", "Elaborate"];
            const responseWords = ["insightful", "concise", "creative", "detailed", "efficient", "accurate", "comprehensive", "innovative", "structured", "relevant"];

            const randomPrompt = promptWords[Math.floor(Math.random() * promptWords.length)] + " " +
                                 promptWords[Math.floor(Math.random() * promptWords.length)].toLowerCase() + ".";
            
            let randomResponse = "";
            for(let i = 0; i < numTokens / 2; i++) { // Generate half the tokens from response words
                randomResponse += responseWords[Math.floor(Math.random() * responseWords.length)] + " ";
            }
            randomResponse += "output generated."; // Add a concluding phrase

            return { prompt: randomPrompt, response: randomResponse.trim() };
        }


        // Main drawing function
        function draw() {
            ctx.clearRect(0, 0, canvas.width, canvas.height); // Clear canvas
            drawEngineCore();

            // Draw particles
            promptFragments.forEach(pf => pf.draw(ctx));
            sparks.forEach(s => s.draw(ctx));
            outputTokens.forEach(ot => ot.draw(ctx));
        }

        // Main animation loop
        function animate(currentTime) {
            if (!lastFrameTime) lastFrameTime = currentTime;
            deltaTime = (currentTime - lastFrameTime) / 1000; // Convert to seconds
            lastFrameTime = currentTime;

            if (!isPaused) {
                // Update particles
                const scaledDt = deltaTime * simulationSpeed;

                promptFragments.forEach((pf, index) => {
                    pf.update(scaledDt);
                    if (pf.reachedTarget) {
                        tokensProcessed++;
                        // Generate sparks when a fragment reaches the core
                        const numSparks = Math.floor(Math.random() * 5) + 3; // 3-7 sparks
                        for (let i = 0; i < numSparks; i++) {
                            const angle = Math.random() * Math.PI * 2;
                            sparks.push(new Spark(pf.x, pf.y, 2 + Math.random() * 2, angle, 80 + Math.random() * 50));
                        }
                        // Remove processed fragment
                        promptFragments.splice(index, 1);
                    }
                });

                sparks.forEach((s, index) => {
                    s.update(scaledDt);
                    if (s.life <= 0) {
                        sparks.splice(index, 1);
                    }
                });

                outputTokens.forEach((ot, index) => {
                    ot.update(scaledDt);
                    if (ot.life <= 0) {
                        outputTokens.splice(index, 1);
                    }
                });

                updateStats();
                updateResourceGauges(); // Call this in the animation loop
            }

            draw(); // Redraw everything

            animationFrameId = requestAnimationFrame(animate);
        }

        // Event Listeners
        submitPromptButton.addEventListener('click', submitPrompt);
        pauseButton.addEventListener('click', () => {
            isPaused = !isPaused;
            pauseButton.textContent = isPaused ? 'Resume Sim' : 'Pause Sim';
            if (!isPaused) {
                lastFrameTime = performance.now(); // Reset lastFrameTime to prevent jump
                animate(lastFrameTime); // Resume animation loop
            } else {
                cancelAnimationFrame(animationFrameId); // Stop animation loop
            }
        });
        resetButton.addEventListener('click', initSimulation);

        // Inference Mode Toggle Buttons
        modeLocalBtn.addEventListener('click', () => {
            inferenceMode = 'local';
            modeLocalBtn.classList.add('active');
            modeCloudBtn.classList.remove('active');
            updateTabContentVisibility();
            updateSimulationParameters();
            updateDescriptions();
        });

        modeCloudBtn.addEventListener('click', () => {
            inferenceMode = 'cloud';
            modeCloudBtn.classList.add('active');
            modeLocalBtn.classList.remove('active');
            updateTabContentVisibility();
            updateSimulationParameters();
            updateDescriptions();
        });


        // Model Type Radio Buttons (Local)
        document.querySelectorAll('input[name="modelType"]').forEach(radio => {
            radio.addEventListener('change', (event) => {
                currentModelType = event.target.value;
                updateSimulationParameters();
                updateDescriptions();
            });
        });

        // Input Type Radio Buttons (Local)
        document.querySelectorAll('input[name="inputType"]').forEach(radio => {
            radio.addEventListener('change', (event) => {
                currentInputType = event.target.value;
                updateSimulationParameters();
                updateDescriptions();
            });
        });

        // Quantization Type Radio Buttons (Local)
        document.querySelectorAll('input[name="quantizationType"]').forEach(radio => {
            radio.addEventListener('change', (event) => {
                currentQuantizationType = event.target.value;
                updateSimulationParameters();
                updateDescriptions();
            });
        });

        // Context Window Slider (Local)
        contextWindowSlider.addEventListener('input', (event) => {
            contextWindowSize = parseInt(event.target.value);
            contextWindowValueElem.textContent = contextWindowSize;
            updateSimulationParameters();
            updateDescriptions();
        });

        // Cloud Model Tier Radio Buttons (Cloud)
        document.querySelectorAll('input[name="cloudModelTier"]').forEach(radio => {
            radio.addEventListener('change', (event) => {
                cloudModelTier = event.target.value;
                updateSimulationParameters();
                updateDescriptions();
            });
        });

        // Network Latency Radio Buttons (Cloud)
        document.querySelectorAll('input[name="networkLatency"]').forEach(radio => {
            radio.addEventListener('change', (event) => {
                networkLatency = event.target.value;
                updateSimulationParameters();
                updateDescriptions();
            });
        });

        // Cloud Context Window Slider (Cloud)
        cloudContextWindowSlider.addEventListener('input', (event) => {
            cloudContextWindow = parseInt(event.target.value);
            cloudContextWindowValueElem.textContent = cloudContextWindow;
            updateSimulationParameters();
            updateDescriptions();
        });


        // Simulation Speed Slider (Applies to both modes)
        simulationSpeedSlider.addEventListener('input', (event) => {
            simulationSpeed = parseFloat(event.target.value);
            simulationSpeedValueElem.textContent = `${simulationSpeed.toFixed(1)}x`;
        });

        // Tab functionality
        document.querySelectorAll('.tab-button').forEach(button => {
            button.addEventListener('click', () => {
                // Deactivate all tabs and hide all content
                document.querySelectorAll('.tab-button').forEach(btn => btn.classList.remove('active'));
                document.querySelectorAll('.tab-content').forEach(content => content.classList.add('hidden')); // Add 'hidden' to all content

                // Activate clicked tab and show its content
                button.classList.add('active');
                document.getElementById(`${button.dataset.tab}-tab`).classList.remove('hidden'); // Remove 'hidden' from active content
                updateTabContentVisibility(); // Ensure correct local/cloud content is shown for the newly active tab
            });
        });

        // Resize canvas on window resize
        window.addEventListener('resize', () => {
            canvas.width = canvas.offsetWidth;
            canvas.height = canvas.offsetHeight;
            draw(); // Redraw content to fit new size
        });

        // Initial setup on page load
        window.onload = function() {
            populatePresetRadios();
            // Defer canvas sizing and simulation init to ensure correct dimensions are read
            requestAnimationFrame(() => {
                canvas.width = canvas.offsetWidth;
                canvas.height = canvas.offsetHeight;
                // initSimulation() will be called after successful password entry
            });
        };

        // Password check logic
        passwordSubmit.addEventListener('click', () => {
            const enteredPassword = passwordInput.value;
            const correctPassword = '123456'; // The required password

            if (enteredPassword === correctPassword) {
                passwordLock.style.display = 'none'; // Hide the password screen
                appContainer.style.display = 'block'; // Show the main app
                initSimulation(); // Initialize the simulation after successful login

                // Manually activate the "Simulation" tab after login
                document.querySelectorAll('.tab-button').forEach(btn => btn.classList.remove('active'));
                document.querySelectorAll('.tab-content').forEach(content => content.classList.add('hidden'));
                document.querySelector('.tab-button[data-tab="simulation"]').classList.add('active');
                document.getElementById('simulation-tab').classList.remove('hidden');

                animate(performance.now()); // Start the animation loop
            } else {
                passwordError.classList.remove('hidden'); // Show error message
                passwordInput.value = ''; // Clear the input
                passwordInput.focus(); // Focus for re-entry
            }
        });

        // Allow pressing Enter key to submit password
        passwordInput.addEventListener('keypress', (event) => {
            if (event.key === 'Enter') {
                passwordSubmit.click();
            }
        });

    </script>
</body>
</html>
